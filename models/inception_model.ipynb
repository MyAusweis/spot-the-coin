{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inception_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vev5DAYLK5OZ",
        "colab_type": "text"
      },
      "source": [
        "# **Bangkit Final Project: World Coin Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aF2mIOQNC5o",
        "colab_type": "text"
      },
      "source": [
        "# **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu8dZbIdLrLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.optimizers import Adam, Nadam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import Regularizer, l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "from google.colab import files, drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0MskslwLsYt",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2245adea-7a21-4b26-8c7a-37f11e431eee"
      },
      "source": [
        "# Upload the kaggle.json file from Kaggle account settings page\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f798f2f-950e-4854-9daf-55db69dcbc37\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f798f2f-950e-4854-9daf-55db69dcbc37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-mZ_XHMezh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the Kaggle API client\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSwixL0MUxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_rjYl-NU0_",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS2VsR0CNg0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4861ce41-63e6-4b69-f9fb-c77d8efff300"
      },
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d wanderdust/coin-images"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coin-images.zip to /content\n",
            " 99% 454M/459M [00:07<00:00, 62.0MB/s]\n",
            "100% 459M/459M [00:07<00:00, 60.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN_bT4N7OopU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If kaggle is down, mount from drive\n",
        "try:\n",
        "    coin_file = open('/content/coin-images.zip', 'r')\n",
        "    filepath = '/content/coin-images.zip'\n",
        "except FileNotFoundError:\n",
        "    # Keep preset values\n",
        "    drive.mount('/content/drive')\n",
        "    filepath = '/content/drive/My Drive/Bangkit project/Dataset/coin-images.zip'\n",
        "\n",
        "# Unzip the dataset into folder\n",
        "zip_ref = zipfile.ZipFile(filepath, 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2d5YlhRQpK",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaQrJvGRTG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define directories\n",
        "data_dir = \"/content/coins/data/\"\n",
        "\n",
        "train_dir = data_dir + \"train/\"\n",
        "validation_dir = data_dir + \"validation/\"\n",
        "test_dir = data_dir + \"test/\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZoeHe2hJ-SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=360,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      brightness_range=[0.8,1.2],\n",
        "      horizontal_flip=False,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      featurewise_center=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      samplewise_center=False,\n",
        "      fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=360,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      brightness_range=[0.8,1.2],\n",
        "      horizontal_flip=False,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      featurewise_center=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      samplewise_center=False,\n",
        "      fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      featurewise_std_normalization=False,\n",
        "      featurewise_center=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      samplewise_center=False\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BDiwC6MJcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "96cae70f-c1fd-4054-f1ce-aebc979d2029"
      },
      "source": [
        "# Read images from generators\n",
        "batch_size = 32\n",
        "image_width = 299\n",
        "image_height = 299\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "      validation_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "      test_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=1\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6413 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9QD8ft22wk_",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1T4tt0OrhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d99b4d08-21ff-41e6-def5-32faa577d684"
      },
      "source": [
        "# Load base model\n",
        "base_model = InceptionV3(input_shape=(image_width, image_height, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "    # Add regularizer\n",
        "    l2_layer = l2(0.01)\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        base_model.add_loss(lambda layer=layer: l2_layer(layer.kernel))\n",
        "\n",
        "for layer in base_model.layers[:10]:\n",
        "\t\tlayer.trainable = False\n",
        "\n",
        "# Custom top classifier for model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.4)(x)\n",
        "predictions = Dense(211, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 149, 149, 32) 864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 149, 149, 32) 96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 149, 149, 32) 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 147, 147, 32) 9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 147, 147, 32) 96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 147, 147, 32) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 147, 147, 64) 18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 147, 147, 64) 192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 147, 147, 64) 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 73, 73, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 73, 73, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 73, 73, 80)   0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 71, 71, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 71, 71, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 71, 71, 192)  0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_102 (BatchN (None, 35, 35, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 35, 35, 64)   0           batch_normalization_102[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 35, 35, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_100 (BatchN (None, 35, 35, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_103 (BatchN (None, 35, 35, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 35, 35, 48)   0           batch_normalization_100[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 35, 35, 96)   0           batch_normalization_103[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 35, 35, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 35, 35, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 35, 35, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_99 (BatchNo (None, 35, 35, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (None, 35, 35, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_104 (BatchN (None, 35, 35, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_105 (BatchN (None, 35, 35, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 35, 35, 64)   0           batch_normalization_99[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 35, 35, 64)   0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 35, 35, 96)   0           batch_normalization_104[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 35, 35, 32)   0           batch_normalization_105[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_109 (BatchN (None, 35, 35, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 35, 35, 64)   0           batch_normalization_109[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 35, 35, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_107 (BatchN (None, 35, 35, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_110 (BatchN (None, 35, 35, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 35, 35, 48)   0           batch_normalization_107[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 35, 35, 96)   0           batch_normalization_110[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 35, 35, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 35, 35, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_106 (BatchN (None, 35, 35, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_108 (BatchN (None, 35, 35, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_111 (BatchN (None, 35, 35, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_112 (BatchN (None, 35, 35, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 35, 35, 64)   0           batch_normalization_106[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 35, 35, 64)   0           batch_normalization_108[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 35, 35, 96)   0           batch_normalization_111[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 35, 35, 64)   0           batch_normalization_112[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_116 (BatchN (None, 35, 35, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 35, 35, 64)   0           batch_normalization_116[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 35, 35, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_114 (BatchN (None, 35, 35, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_117 (BatchN (None, 35, 35, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 35, 35, 48)   0           batch_normalization_114[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 35, 35, 96)   0           batch_normalization_117[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 35, 35, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 35, 35, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_113 (BatchN (None, 35, 35, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_115 (BatchN (None, 35, 35, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_118 (BatchN (None, 35, 35, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_119 (BatchN (None, 35, 35, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 35, 35, 64)   0           batch_normalization_113[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 35, 35, 64)   0           batch_normalization_115[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 35, 35, 96)   0           batch_normalization_118[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 35, 35, 64)   0           batch_normalization_119[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_121 (BatchN (None, 35, 35, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 35, 35, 64)   0           batch_normalization_121[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 35, 35, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_122 (BatchN (None, 35, 35, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 35, 35, 96)   0           batch_normalization_122[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 17, 17, 96)   82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_120 (BatchN (None, 17, 17, 384)  1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_123 (BatchN (None, 17, 17, 96)   288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 17, 17, 384)  0           batch_normalization_120[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 17, 17, 96)   0           batch_normalization_123[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_128 (BatchN (None, 17, 17, 128)  384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 17, 17, 128)  0           batch_normalization_128[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 17, 17, 128)  114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_129 (BatchN (None, 17, 17, 128)  384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 17, 17, 128)  0           batch_normalization_129[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 17, 17, 128)  114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_125 (BatchN (None, 17, 17, 128)  384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_130 (BatchN (None, 17, 17, 128)  384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 17, 17, 128)  0           batch_normalization_125[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 17, 17, 128)  0           batch_normalization_130[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 17, 17, 128)  114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 17, 17, 128)  114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_126 (BatchN (None, 17, 17, 128)  384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_131 (BatchN (None, 17, 17, 128)  384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 17, 17, 128)  0           batch_normalization_126[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 17, 17, 128)  0           batch_normalization_131[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 17, 17, 192)  172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 17, 17, 192)  172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_124 (BatchN (None, 17, 17, 192)  576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_127 (BatchN (None, 17, 17, 192)  576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_132 (BatchN (None, 17, 17, 192)  576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_133 (BatchN (None, 17, 17, 192)  576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 17, 17, 192)  0           batch_normalization_124[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 17, 17, 192)  0           batch_normalization_127[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 17, 17, 192)  0           batch_normalization_132[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 17, 17, 192)  0           batch_normalization_133[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_138 (BatchN (None, 17, 17, 160)  480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 17, 17, 160)  0           batch_normalization_138[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 17, 17, 160)  179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_139 (BatchN (None, 17, 17, 160)  480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 17, 17, 160)  0           batch_normalization_139[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 17, 17, 160)  179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_135 (BatchN (None, 17, 17, 160)  480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_140 (BatchN (None, 17, 17, 160)  480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 17, 17, 160)  0           batch_normalization_135[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 17, 17, 160)  0           batch_normalization_140[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 17, 17, 160)  179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 17, 17, 160)  179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_136 (BatchN (None, 17, 17, 160)  480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_141 (BatchN (None, 17, 17, 160)  480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 17, 17, 160)  0           batch_normalization_136[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 17, 17, 160)  0           batch_normalization_141[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 17, 17, 192)  215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 17, 17, 192)  215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_134 (BatchN (None, 17, 17, 192)  576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_137 (BatchN (None, 17, 17, 192)  576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_142 (BatchN (None, 17, 17, 192)  576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_143 (BatchN (None, 17, 17, 192)  576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 17, 17, 192)  0           batch_normalization_134[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 17, 17, 192)  0           batch_normalization_137[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 17, 17, 192)  0           batch_normalization_142[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 17, 17, 192)  0           batch_normalization_143[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_148 (BatchN (None, 17, 17, 160)  480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 17, 17, 160)  0           batch_normalization_148[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 17, 17, 160)  179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_149 (BatchN (None, 17, 17, 160)  480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 17, 17, 160)  0           batch_normalization_149[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 17, 17, 160)  179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_145 (BatchN (None, 17, 17, 160)  480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_150 (BatchN (None, 17, 17, 160)  480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 17, 17, 160)  0           batch_normalization_145[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 17, 17, 160)  0           batch_normalization_150[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 17, 17, 160)  179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 17, 17, 160)  179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_146 (BatchN (None, 17, 17, 160)  480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_151 (BatchN (None, 17, 17, 160)  480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 17, 17, 160)  0           batch_normalization_146[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 17, 17, 160)  0           batch_normalization_151[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 17, 17, 192)  215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 17, 17, 192)  215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_144 (BatchN (None, 17, 17, 192)  576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_147 (BatchN (None, 17, 17, 192)  576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_152 (BatchN (None, 17, 17, 192)  576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_153 (BatchN (None, 17, 17, 192)  576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 17, 17, 192)  0           batch_normalization_144[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 17, 17, 192)  0           batch_normalization_147[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 17, 17, 192)  0           batch_normalization_152[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 17, 17, 192)  0           batch_normalization_153[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_158 (BatchN (None, 17, 17, 192)  576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 17, 17, 192)  0           batch_normalization_158[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 17, 17, 192)  258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_159 (BatchN (None, 17, 17, 192)  576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 17, 17, 192)  0           batch_normalization_159[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 17, 17, 192)  258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_155 (BatchN (None, 17, 17, 192)  576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_160 (BatchN (None, 17, 17, 192)  576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 17, 17, 192)  0           batch_normalization_155[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 17, 17, 192)  0           batch_normalization_160[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 17, 17, 192)  258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 17, 17, 192)  258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_156 (BatchN (None, 17, 17, 192)  576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_161 (BatchN (None, 17, 17, 192)  576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 17, 17, 192)  0           batch_normalization_156[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 17, 17, 192)  0           batch_normalization_161[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 17, 17, 192)  258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 17, 17, 192)  258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_154 (BatchN (None, 17, 17, 192)  576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_157 (BatchN (None, 17, 17, 192)  576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_162 (BatchN (None, 17, 17, 192)  576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_163 (BatchN (None, 17, 17, 192)  576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 17, 17, 192)  0           batch_normalization_154[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 17, 17, 192)  0           batch_normalization_157[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 17, 17, 192)  0           batch_normalization_162[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 17, 17, 192)  0           batch_normalization_163[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_166 (BatchN (None, 17, 17, 192)  576         conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_166 (Activation)     (None, 17, 17, 192)  0           batch_normalization_166[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 17, 17, 192)  258048      activation_166[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_167 (BatchN (None, 17, 17, 192)  576         conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_167 (Activation)     (None, 17, 17, 192)  0           batch_normalization_167[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 17, 17, 192)  258048      activation_167[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_164 (BatchN (None, 17, 17, 192)  576         conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_168 (BatchN (None, 17, 17, 192)  576         conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_164 (Activation)     (None, 17, 17, 192)  0           batch_normalization_164[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_168 (Activation)     (None, 17, 17, 192)  0           batch_normalization_168[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 8, 8, 320)    552960      activation_164[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 8, 8, 192)    331776      activation_168[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_165 (BatchN (None, 8, 8, 320)    960         conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_169 (BatchN (None, 8, 8, 192)    576         conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_165 (Activation)     (None, 8, 8, 320)    0           batch_normalization_165[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_169 (Activation)     (None, 8, 8, 192)    0           batch_normalization_169[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_165[0][0]             \n",
            "                                                                 activation_169[0][0]             \n",
            "                                                                 max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_174 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_174 (BatchN (None, 8, 8, 448)    1344        conv2d_174[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_174 (Activation)     (None, 8, 8, 448)    0           batch_normalization_174[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_175 (Conv2D)             (None, 8, 8, 384)    1548288     activation_174[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_171 (BatchN (None, 8, 8, 384)    1152        conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_175 (BatchN (None, 8, 8, 384)    1152        conv2d_175[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_171 (Activation)     (None, 8, 8, 384)    0           batch_normalization_171[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_175 (Activation)     (None, 8, 8, 384)    0           batch_normalization_175[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_172 (Conv2D)             (None, 8, 8, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_173 (Conv2D)             (None, 8, 8, 384)    442368      activation_171[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_176 (Conv2D)             (None, 8, 8, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_177 (Conv2D)             (None, 8, 8, 384)    442368      activation_175[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_16 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_172 (BatchN (None, 8, 8, 384)    1152        conv2d_172[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_173 (BatchN (None, 8, 8, 384)    1152        conv2d_173[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_176 (BatchN (None, 8, 8, 384)    1152        conv2d_176[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_177 (BatchN (None, 8, 8, 384)    1152        conv2d_177[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_178 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_16[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_170 (BatchN (None, 8, 8, 320)    960         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_172 (Activation)     (None, 8, 8, 384)    0           batch_normalization_172[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_173 (Activation)     (None, 8, 8, 384)    0           batch_normalization_173[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_176 (Activation)     (None, 8, 8, 384)    0           batch_normalization_176[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_177 (Activation)     (None, 8, 8, 384)    0           batch_normalization_177[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_178 (BatchN (None, 8, 8, 192)    576         conv2d_178[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_170 (Activation)     (None, 8, 8, 320)    0           batch_normalization_170[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_172[0][0]             \n",
            "                                                                 activation_173[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_176[0][0]             \n",
            "                                                                 activation_177[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_178 (Activation)     (None, 8, 8, 192)    0           batch_normalization_178[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_170[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_178[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_183 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_183 (BatchN (None, 8, 8, 448)    1344        conv2d_183[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_183 (Activation)     (None, 8, 8, 448)    0           batch_normalization_183[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_180 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_184 (Conv2D)             (None, 8, 8, 384)    1548288     activation_183[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_180 (BatchN (None, 8, 8, 384)    1152        conv2d_180[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_184 (BatchN (None, 8, 8, 384)    1152        conv2d_184[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_180 (Activation)     (None, 8, 8, 384)    0           batch_normalization_180[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_184 (Activation)     (None, 8, 8, 384)    0           batch_normalization_184[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_181 (Conv2D)             (None, 8, 8, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_182 (Conv2D)             (None, 8, 8, 384)    442368      activation_180[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_185 (Conv2D)             (None, 8, 8, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_186 (Conv2D)             (None, 8, 8, 384)    442368      activation_184[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_17 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_179 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_181 (BatchN (None, 8, 8, 384)    1152        conv2d_181[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_182 (BatchN (None, 8, 8, 384)    1152        conv2d_182[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_185 (BatchN (None, 8, 8, 384)    1152        conv2d_185[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_186 (BatchN (None, 8, 8, 384)    1152        conv2d_186[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_187 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_17[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_179 (BatchN (None, 8, 8, 320)    960         conv2d_179[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_181 (Activation)     (None, 8, 8, 384)    0           batch_normalization_181[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_182 (Activation)     (None, 8, 8, 384)    0           batch_normalization_182[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_185 (Activation)     (None, 8, 8, 384)    0           batch_normalization_185[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_186 (Activation)     (None, 8, 8, 384)    0           batch_normalization_186[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_187 (BatchN (None, 8, 8, 192)    576         conv2d_187[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_179 (Activation)     (None, 8, 8, 320)    0           batch_normalization_179[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_181[0][0]             \n",
            "                                                                 activation_182[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 8, 8, 768)    0           activation_185[0][0]             \n",
            "                                                                 activation_186[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_187 (Activation)     (None, 8, 8, 192)    0           batch_normalization_187[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_179[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_3[0][0]              \n",
            "                                                                 activation_187[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          1049088     global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 211)          108243      dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 22,960,115\n",
            "Trainable params: 22,897,043\n",
            "Non-trainable params: 63,072\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnhZ9bR-PiVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fb9424c-9e89-4bb5-8d5a-6acb5ec55c42"
      },
      "source": [
        "# Callback to reduce learning rate if no improvement in validation loss for certain number of epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-8, verbose=1)\n",
        "\n",
        "# Callback to stop training if no improvement in validation loss for certain number of epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# Callback to save best model weights per epoch\n",
        "weights_filepath = \"best_model_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=weights_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Nadam(lr=0.0001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=80,\n",
        "    steps_per_epoch=50,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    validation_steps=3,\n",
        "    callbacks=[reduce_lr, checkpoint]\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.3361 - accuracy: 0.0200\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.02083, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 49s 979ms/step - loss: 5.3361 - accuracy: 0.0200 - val_loss: 5.3155 - val_accuracy: 0.0208 - lr: 1.0000e-04\n",
            "Epoch 2/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.9635 - accuracy: 0.0794\n",
            "Epoch 00002: val_accuracy improved from 0.02083 to 0.11458, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 959ms/step - loss: 4.9635 - accuracy: 0.0794 - val_loss: 4.8224 - val_accuracy: 0.1146 - lr: 1.0000e-04\n",
            "Epoch 3/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.3715 - accuracy: 0.1800\n",
            "Epoch 00003: val_accuracy improved from 0.11458 to 0.16667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 964ms/step - loss: 4.3715 - accuracy: 0.1800 - val_loss: 4.3390 - val_accuracy: 0.1667 - lr: 1.0000e-04\n",
            "Epoch 4/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.8302 - accuracy: 0.2450\n",
            "Epoch 00004: val_accuracy improved from 0.16667 to 0.17708, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 957ms/step - loss: 3.8302 - accuracy: 0.2450 - val_loss: 3.9201 - val_accuracy: 0.1771 - lr: 1.0000e-04\n",
            "Epoch 5/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.2688 - accuracy: 0.3644\n",
            "Epoch 00005: val_accuracy improved from 0.17708 to 0.32292, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 962ms/step - loss: 3.2688 - accuracy: 0.3644 - val_loss: 3.0952 - val_accuracy: 0.3229 - lr: 1.0000e-04\n",
            "Epoch 6/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.8396 - accuracy: 0.4269\n",
            "Epoch 00006: val_accuracy improved from 0.32292 to 0.41667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 959ms/step - loss: 2.8396 - accuracy: 0.4269 - val_loss: 2.6714 - val_accuracy: 0.4167 - lr: 1.0000e-04\n",
            "Epoch 7/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.5110 - accuracy: 0.4725\n",
            "Epoch 00007: val_accuracy improved from 0.41667 to 0.48958, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 959ms/step - loss: 2.5110 - accuracy: 0.4725 - val_loss: 2.4646 - val_accuracy: 0.4896 - lr: 1.0000e-04\n",
            "Epoch 8/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.1963 - accuracy: 0.5288\n",
            "Epoch 00008: val_accuracy improved from 0.48958 to 0.60417, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 951ms/step - loss: 2.1963 - accuracy: 0.5288 - val_loss: 2.2035 - val_accuracy: 0.6042 - lr: 1.0000e-04\n",
            "Epoch 9/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.7690 - accuracy: 0.6287\n",
            "Epoch 00009: val_accuracy improved from 0.60417 to 0.61458, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 47s 939ms/step - loss: 1.7690 - accuracy: 0.6287 - val_loss: 1.9302 - val_accuracy: 0.6146 - lr: 1.0000e-04\n",
            "Epoch 10/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.7164 - accuracy: 0.6275\n",
            "Epoch 00010: val_accuracy improved from 0.61458 to 0.65625, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 966ms/step - loss: 1.7164 - accuracy: 0.6275 - val_loss: 1.7118 - val_accuracy: 0.6562 - lr: 1.0000e-04\n",
            "Epoch 11/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.4763 - accuracy: 0.6725\n",
            "Epoch 00011: val_accuracy improved from 0.65625 to 0.67708, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 954ms/step - loss: 1.4763 - accuracy: 0.6725 - val_loss: 1.3902 - val_accuracy: 0.6771 - lr: 1.0000e-04\n",
            "Epoch 12/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.3294 - accuracy: 0.6964\n",
            "Epoch 00012: val_accuracy did not improve from 0.67708\n",
            "50/50 [==============================] - 45s 909ms/step - loss: 1.3294 - accuracy: 0.6964 - val_loss: 1.4521 - val_accuracy: 0.6667 - lr: 1.0000e-04\n",
            "Epoch 13/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2143 - accuracy: 0.7244\n",
            "Epoch 00013: val_accuracy did not improve from 0.67708\n",
            "50/50 [==============================] - 46s 927ms/step - loss: 1.2143 - accuracy: 0.7244 - val_loss: 1.3498 - val_accuracy: 0.6667 - lr: 1.0000e-04\n",
            "Epoch 14/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.1379 - accuracy: 0.7450\n",
            "Epoch 00014: val_accuracy improved from 0.67708 to 0.71875, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 956ms/step - loss: 1.1379 - accuracy: 0.7450 - val_loss: 1.1364 - val_accuracy: 0.7188 - lr: 1.0000e-04\n",
            "Epoch 15/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0208 - accuracy: 0.7700\n",
            "Epoch 00015: val_accuracy did not improve from 0.71875\n",
            "50/50 [==============================] - 46s 925ms/step - loss: 1.0208 - accuracy: 0.7700 - val_loss: 0.9850 - val_accuracy: 0.7083 - lr: 1.0000e-04\n",
            "Epoch 16/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.7594\n",
            "Epoch 00016: val_accuracy improved from 0.71875 to 0.73958, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 48s 951ms/step - loss: 1.0021 - accuracy: 0.7594 - val_loss: 0.9185 - val_accuracy: 0.7396 - lr: 1.0000e-04\n",
            "Epoch 17/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8940 - accuracy: 0.7825\n",
            "Epoch 00017: val_accuracy did not improve from 0.73958\n",
            "50/50 [==============================] - 46s 925ms/step - loss: 0.8940 - accuracy: 0.7825 - val_loss: 1.0610 - val_accuracy: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 18/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8376 - accuracy: 0.7900\n",
            "Epoch 00018: val_accuracy improved from 0.73958 to 0.80208, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 47s 939ms/step - loss: 0.8376 - accuracy: 0.7900 - val_loss: 1.0060 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 19/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7794 - accuracy: 0.8163\n",
            "Epoch 00019: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 46s 912ms/step - loss: 0.7794 - accuracy: 0.8163 - val_loss: 0.9948 - val_accuracy: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 20/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7789 - accuracy: 0.8039\n",
            "Epoch 00020: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 895ms/step - loss: 0.7789 - accuracy: 0.8039 - val_loss: 0.8367 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 21/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.8305\n",
            "Epoch 00021: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 897ms/step - loss: 0.6898 - accuracy: 0.8305 - val_loss: 0.7299 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 22/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6473 - accuracy: 0.8356\n",
            "Epoch 00022: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 906ms/step - loss: 0.6473 - accuracy: 0.8356 - val_loss: 0.9996 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 23/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6102 - accuracy: 0.8475\n",
            "Epoch 00023: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 46s 915ms/step - loss: 0.6102 - accuracy: 0.8475 - val_loss: 0.8850 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 24/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6136 - accuracy: 0.8450\n",
            "Epoch 00024: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 905ms/step - loss: 0.6136 - accuracy: 0.8450 - val_loss: 0.9270 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 25/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5602 - accuracy: 0.8587\n",
            "Epoch 00025: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 47s 935ms/step - loss: 0.5602 - accuracy: 0.8587 - val_loss: 0.8341 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 26/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.8794\n",
            "Epoch 00026: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 46s 917ms/step - loss: 0.4775 - accuracy: 0.8794 - val_loss: 1.0091 - val_accuracy: 0.7396 - lr: 1.0000e-04\n",
            "Epoch 27/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.8612\n",
            "Epoch 00027: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 46s 919ms/step - loss: 0.5410 - accuracy: 0.8612 - val_loss: 0.9055 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 28/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4325 - accuracy: 0.8913\n",
            "Epoch 00028: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 906ms/step - loss: 0.4325 - accuracy: 0.8913 - val_loss: 0.8588 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 29/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4557 - accuracy: 0.8881\n",
            "Epoch 00029: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 900ms/step - loss: 0.4557 - accuracy: 0.8881 - val_loss: 0.7635 - val_accuracy: 0.7917 - lr: 1.0000e-04\n",
            "Epoch 30/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.8893\n",
            "Epoch 00030: val_accuracy did not improve from 0.80208\n",
            "50/50 [==============================] - 45s 899ms/step - loss: 0.4465 - accuracy: 0.8893 - val_loss: 0.8902 - val_accuracy: 0.7708 - lr: 1.0000e-04\n",
            "Epoch 31/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4136 - accuracy: 0.8869\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.80208 to 0.82292, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 46s 925ms/step - loss: 0.4136 - accuracy: 0.8869 - val_loss: 0.7424 - val_accuracy: 0.8229 - lr: 1.0000e-04\n",
            "Epoch 32/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.9087\n",
            "Epoch 00032: val_accuracy did not improve from 0.82292\n",
            "50/50 [==============================] - 45s 903ms/step - loss: 0.3484 - accuracy: 0.9087 - val_loss: 1.1117 - val_accuracy: 0.7500 - lr: 1.0000e-05\n",
            "Epoch 33/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3628 - accuracy: 0.9056\n",
            "Epoch 00033: val_accuracy did not improve from 0.82292\n",
            "50/50 [==============================] - 45s 907ms/step - loss: 0.3628 - accuracy: 0.9056 - val_loss: 0.9977 - val_accuracy: 0.7917 - lr: 1.0000e-05\n",
            "Epoch 34/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.9087\n",
            "Epoch 00034: val_accuracy improved from 0.82292 to 0.83333, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 46s 929ms/step - loss: 0.3506 - accuracy: 0.9087 - val_loss: 0.6624 - val_accuracy: 0.8333 - lr: 1.0000e-05\n",
            "Epoch 35/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.9237\n",
            "Epoch 00035: val_accuracy improved from 0.83333 to 0.85417, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 46s 927ms/step - loss: 0.2913 - accuracy: 0.9237 - val_loss: 0.6472 - val_accuracy: 0.8542 - lr: 1.0000e-05\n",
            "Epoch 36/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9256\n",
            "Epoch 00036: val_accuracy improved from 0.85417 to 0.87500, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 46s 920ms/step - loss: 0.3017 - accuracy: 0.9256 - val_loss: 0.5450 - val_accuracy: 0.8750 - lr: 1.0000e-05\n",
            "Epoch 37/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.9159\n",
            "Epoch 00037: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 892ms/step - loss: 0.3026 - accuracy: 0.9159 - val_loss: 0.5626 - val_accuracy: 0.8542 - lr: 1.0000e-05\n",
            "Epoch 38/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9431\n",
            "Epoch 00038: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 894ms/step - loss: 0.2573 - accuracy: 0.9431 - val_loss: 0.8056 - val_accuracy: 0.8229 - lr: 1.0000e-05\n",
            "Epoch 39/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2811 - accuracy: 0.9200\n",
            "Epoch 00039: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 917ms/step - loss: 0.2811 - accuracy: 0.9200 - val_loss: 0.7823 - val_accuracy: 0.8646 - lr: 1.0000e-05\n",
            "Epoch 40/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9319\n",
            "Epoch 00040: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 909ms/step - loss: 0.2842 - accuracy: 0.9319 - val_loss: 0.5519 - val_accuracy: 0.8750 - lr: 1.0000e-05\n",
            "Epoch 41/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9431\n",
            "Epoch 00041: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 909ms/step - loss: 0.2459 - accuracy: 0.9431 - val_loss: 0.7762 - val_accuracy: 0.8229 - lr: 1.0000e-05\n",
            "Epoch 42/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9400\n",
            "Epoch 00042: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 910ms/step - loss: 0.2536 - accuracy: 0.9400 - val_loss: 0.6316 - val_accuracy: 0.8542 - lr: 1.0000e-05\n",
            "Epoch 43/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2515 - accuracy: 0.9337\n",
            "Epoch 00043: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 909ms/step - loss: 0.2515 - accuracy: 0.9337 - val_loss: 0.7045 - val_accuracy: 0.8125 - lr: 1.0000e-05\n",
            "Epoch 44/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9285\n",
            "Epoch 00044: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 45s 904ms/step - loss: 0.2634 - accuracy: 0.9285 - val_loss: 1.0103 - val_accuracy: 0.7917 - lr: 1.0000e-05\n",
            "Epoch 45/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2463 - accuracy: 0.9400\n",
            "Epoch 00045: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 914ms/step - loss: 0.2463 - accuracy: 0.9400 - val_loss: 0.6189 - val_accuracy: 0.8646 - lr: 1.0000e-05\n",
            "Epoch 46/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9419\n",
            "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 918ms/step - loss: 0.2289 - accuracy: 0.9419 - val_loss: 0.5986 - val_accuracy: 0.8646 - lr: 1.0000e-05\n",
            "Epoch 47/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2220 - accuracy: 0.9419\n",
            "Epoch 00047: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 913ms/step - loss: 0.2220 - accuracy: 0.9419 - val_loss: 0.8253 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 48/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9419\n",
            "Epoch 00048: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 911ms/step - loss: 0.2145 - accuracy: 0.9419 - val_loss: 0.6824 - val_accuracy: 0.8333 - lr: 1.0000e-06\n",
            "Epoch 49/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2072 - accuracy: 0.9469\n",
            "Epoch 00049: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 915ms/step - loss: 0.2072 - accuracy: 0.9469 - val_loss: 0.7590 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 50/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9494\n",
            "Epoch 00050: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 46s 920ms/step - loss: 0.2047 - accuracy: 0.9494 - val_loss: 0.5439 - val_accuracy: 0.8750 - lr: 1.0000e-06\n",
            "Epoch 51/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9300\n",
            "Epoch 00051: val_accuracy improved from 0.87500 to 0.91667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 47s 933ms/step - loss: 0.2419 - accuracy: 0.9300 - val_loss: 0.3576 - val_accuracy: 0.9167 - lr: 1.0000e-06\n",
            "Epoch 52/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9399\n",
            "Epoch 00052: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 896ms/step - loss: 0.2430 - accuracy: 0.9399 - val_loss: 0.7277 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 53/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2271 - accuracy: 0.9394\n",
            "Epoch 00053: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 909ms/step - loss: 0.2271 - accuracy: 0.9394 - val_loss: 0.9007 - val_accuracy: 0.8021 - lr: 1.0000e-06\n",
            "Epoch 54/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9330\n",
            "Epoch 00054: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 904ms/step - loss: 0.2518 - accuracy: 0.9330 - val_loss: 0.8813 - val_accuracy: 0.7812 - lr: 1.0000e-06\n",
            "Epoch 55/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2119 - accuracy: 0.9475\n",
            "Epoch 00055: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 912ms/step - loss: 0.2119 - accuracy: 0.9475 - val_loss: 0.8081 - val_accuracy: 0.7917 - lr: 1.0000e-06\n",
            "Epoch 56/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9469\n",
            "Epoch 00056: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 912ms/step - loss: 0.2227 - accuracy: 0.9469 - val_loss: 0.8391 - val_accuracy: 0.7917 - lr: 1.0000e-06\n",
            "Epoch 57/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2351 - accuracy: 0.9399\n",
            "Epoch 00057: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 906ms/step - loss: 0.2351 - accuracy: 0.9399 - val_loss: 0.7411 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 58/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9331\n",
            "Epoch 00058: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 912ms/step - loss: 0.2365 - accuracy: 0.9331 - val_loss: 0.7260 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 59/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9481\n",
            "Epoch 00059: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 914ms/step - loss: 0.2163 - accuracy: 0.9481 - val_loss: 0.4189 - val_accuracy: 0.8646 - lr: 1.0000e-06\n",
            "Epoch 60/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9367\n",
            "Epoch 00060: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 899ms/step - loss: 0.2206 - accuracy: 0.9367 - val_loss: 0.5775 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 61/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.9394\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 911ms/step - loss: 0.2465 - accuracy: 0.9394 - val_loss: 0.4836 - val_accuracy: 0.8438 - lr: 1.0000e-06\n",
            "Epoch 62/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9425\n",
            "Epoch 00062: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 903ms/step - loss: 0.2065 - accuracy: 0.9425 - val_loss: 0.6162 - val_accuracy: 0.7604 - lr: 1.0000e-07\n",
            "Epoch 63/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9393\n",
            "Epoch 00063: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 911ms/step - loss: 0.2327 - accuracy: 0.9393 - val_loss: 0.7958 - val_accuracy: 0.8021 - lr: 1.0000e-07\n",
            "Epoch 64/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2233 - accuracy: 0.9388\n",
            "Epoch 00064: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 47s 932ms/step - loss: 0.2233 - accuracy: 0.9388 - val_loss: 0.7373 - val_accuracy: 0.8333 - lr: 1.0000e-07\n",
            "Epoch 65/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9544\n",
            "Epoch 00065: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 923ms/step - loss: 0.1981 - accuracy: 0.9544 - val_loss: 0.7696 - val_accuracy: 0.8542 - lr: 1.0000e-07\n",
            "Epoch 66/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2230 - accuracy: 0.9462\n",
            "Epoch 00066: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 907ms/step - loss: 0.2230 - accuracy: 0.9462 - val_loss: 0.7644 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 67/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9506\n",
            "Epoch 00067: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 915ms/step - loss: 0.2132 - accuracy: 0.9506 - val_loss: 0.9065 - val_accuracy: 0.7812 - lr: 1.0000e-07\n",
            "Epoch 68/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9519\n",
            "Epoch 00068: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 914ms/step - loss: 0.1995 - accuracy: 0.9519 - val_loss: 0.6981 - val_accuracy: 0.8125 - lr: 1.0000e-07\n",
            "Epoch 69/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2027 - accuracy: 0.9462\n",
            "Epoch 00069: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 901ms/step - loss: 0.2027 - accuracy: 0.9462 - val_loss: 0.9683 - val_accuracy: 0.6979 - lr: 1.0000e-07\n",
            "Epoch 70/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9406\n",
            "Epoch 00070: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 919ms/step - loss: 0.2313 - accuracy: 0.9406 - val_loss: 0.5379 - val_accuracy: 0.8854 - lr: 1.0000e-07\n",
            "Epoch 71/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9388\n",
            "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 918ms/step - loss: 0.2299 - accuracy: 0.9388 - val_loss: 0.8994 - val_accuracy: 0.7708 - lr: 1.0000e-07\n",
            "Epoch 72/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9399\n",
            "Epoch 00072: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 903ms/step - loss: 0.2288 - accuracy: 0.9399 - val_loss: 0.5360 - val_accuracy: 0.8854 - lr: 1.0000e-08\n",
            "Epoch 73/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.9388\n",
            "Epoch 00073: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 912ms/step - loss: 0.2282 - accuracy: 0.9388 - val_loss: 0.4006 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 74/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2290 - accuracy: 0.9344\n",
            "Epoch 00074: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 907ms/step - loss: 0.2290 - accuracy: 0.9344 - val_loss: 0.3612 - val_accuracy: 0.9167 - lr: 1.0000e-08\n",
            "Epoch 75/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9355\n",
            "Epoch 00075: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 45s 904ms/step - loss: 0.2506 - accuracy: 0.9355 - val_loss: 0.7427 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 76/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9369\n",
            "Epoch 00076: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 923ms/step - loss: 0.2111 - accuracy: 0.9369 - val_loss: 0.6695 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 77/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9488\n",
            "Epoch 00077: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 924ms/step - loss: 0.2269 - accuracy: 0.9488 - val_loss: 0.6916 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 78/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2362 - accuracy: 0.9362\n",
            "Epoch 00078: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 923ms/step - loss: 0.2362 - accuracy: 0.9362 - val_loss: 0.4372 - val_accuracy: 0.8542 - lr: 1.0000e-08\n",
            "Epoch 79/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9463\n",
            "Epoch 00079: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 46s 922ms/step - loss: 0.2093 - accuracy: 0.9463 - val_loss: 0.5959 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 80/80\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9456\n",
            "Epoch 00080: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 47s 932ms/step - loss: 0.2232 - accuracy: 0.9456 - val_loss: 0.7532 - val_accuracy: 0.8021 - lr: 1.0000e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZTPuGY8rWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "c6dcd386-e764-4e76-b3b2-3d7fad85b494"
      },
      "source": [
        "# Visualise accuracy history\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Visualise loss history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIvbXL32hN48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save model\n",
        "model.save(\"inception_model.h5\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCXE1Nm45huL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "24d916ec-63cc-433c-877e-f58222d65332"
      },
      "source": [
        "# Evaluate model\n",
        "train_score = model.evaluate(train_generator, verbose=1)\n",
        "print(\"Training loss: \", train_score[0])\n",
        "print(\"Training accuracy: \", train_score[1])\n",
        "\n",
        "validation_score = model.evaluate(validation_generator, verbose=1)\n",
        "print(\"Validation loss: \", validation_score[0])\n",
        "print(\"Validation accuracy: \", validation_score[1])\n",
        "\n",
        "test_score = model.evaluate(test_generator, verbose=1)\n",
        "print(\"Testing loss: \", test_score[0])\n",
        "print(\"Testing accuracy: \", test_score[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 159s 791ms/step - loss: 0.0959 - accuracy: 0.9704\n",
            "Training loss:  0.09589181840419769\n",
            "Training accuracy:  0.9703726768493652\n",
            "27/27 [==============================] - 20s 754ms/step - loss: 0.6597 - accuracy: 0.8341\n",
            "Validation loss:  0.6597456932067871\n",
            "Validation accuracy:  0.8341231942176819\n",
            "844/844 [==============================] - 15s 17ms/step - loss: 0.6086 - accuracy: 0.8566\n",
            "Testing loss:  0.6086258292198181\n",
            "Testing accuracy:  0.8566350936889648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ztC3j9zTRyES",
        "colab": {}
      },
      "source": [
        "model_best_weights = model\n",
        "\n",
        "# Load best model weights\n",
        "model_best_weights.load_weights(weights_filepath)\n",
        "\n",
        "# Save model\n",
        "model_best_weights.save(\"inception_model_best_weights.h5\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ed6NgxyJR_Ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "6b53be7c-628f-4cae-a420-861c3c334900"
      },
      "source": [
        "# Evaluate model\n",
        "train_score = model_best_weights.evaluate(train_generator, verbose=1)\n",
        "print(\"Training loss: \", train_score[0])\n",
        "print(\"Training accuracy: \", train_score[1])\n",
        "\n",
        "validation_score = model_best_weights.evaluate(validation_generator, verbose=1)\n",
        "print(\"Validation loss: \", validation_score[0])\n",
        "print(\"Validation accuracy: \", validation_score[1])\n",
        "\n",
        "test_score = model_best_weights.evaluate(test_generator, verbose=1)\n",
        "print(\"Testing loss: \", test_score[0])\n",
        "print(\"Testing accuracy: \", test_score[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 158s 785ms/step - loss: 0.1019 - accuracy: 0.9705\n",
            "Training loss:  0.10186693072319031\n",
            "Training accuracy:  0.9705286026000977\n",
            "27/27 [==============================] - 20s 744ms/step - loss: 0.6898 - accuracy: 0.8329\n",
            "Validation loss:  0.6897547841072083\n",
            "Validation accuracy:  0.8329383730888367\n",
            "844/844 [==============================] - 14s 17ms/step - loss: 0.6142 - accuracy: 0.8566\n",
            "Testing loss:  0.6141502261161804\n",
            "Testing accuracy:  0.8566350936889648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PJ6NpZE8AzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict test images\n",
        "predictions = []\n",
        "\n",
        "for filename in test_generator.filenames:\n",
        "    img = load_img(test_dir+filename, target_size=(image_width, image_height))\n",
        "    img = img_to_array(img)/255\n",
        "    img_expand = np.expand_dims(img, axis=0)\n",
        "    predictions.append(model.predict(img_expand)[0])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHvvZGaHKQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get index of largest probability\n",
        "predicted_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get coin directory name from index \n",
        "directories = dict((v, k) for k, v in train_generator.class_indices.items())\n",
        "predicted_dir = [directories.get(k) for k in predicted_indices]\n",
        "\n",
        "# Get label name from coin directory name\n",
        "with open(data_dir + 'cat_to_name.json', 'r') as json_file:\n",
        "    labels = json.load(json_file)\n",
        "predicted_labels = [labels.get(str(k)) for k in predicted_dir]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4DLt4o3H78s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5f1747e5-af85-444d-e8b6-f95a24e1e495"
      },
      "source": [
        "# Save predicted labels as CSV file\n",
        "filenames = test_generator.filenames\n",
        "results = pd.DataFrame({\"Filename\": filenames, \"Predictions\": predicted_labels})\n",
        "results.to_csv(\"inception_results.csv\", index=False)\n",
        "results.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/021__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/022__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/027__1 Cent_australia.jpg</td>\n",
              "      <td>2 Cents,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/036__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/005__5 Centavos_brazil.jpg</td>\n",
              "      <td>5 Centavos,Brazilian Real,brazil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Filename                          Predictions\n",
              "0    1/021__1 Cent_australia.jpg   1 Cent,Australian dollar,australia\n",
              "1    1/022__1 Cent_australia.jpg   1 Cent,Australian dollar,australia\n",
              "2    1/027__1 Cent_australia.jpg  2 Cents,Australian dollar,australia\n",
              "3    1/036__1 Cent_australia.jpg   1 Cent,Australian dollar,australia\n",
              "4  10/005__5 Centavos_brazil.jpg     5 Centavos,Brazilian Real,brazil"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlj71Kl_2WO",
        "colab_type": "text"
      },
      "source": [
        "# **Convert to TFLite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVWQuD08_1hc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab41fd11-d75e-4ad0-ecfd-3b9c754cd4b1"
      },
      "source": [
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "open(\"inception_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23018208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX-yMWB34O4C",
        "colab_type": "text"
      },
      "source": [
        "# **Copy model to Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcyZNaSi4bkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6d27a1a7-0887-4064-80ca-a794efc61064"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "!cp inception_model.h5 \"/content/drive/My Drive/Bangkit project/models\""
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}