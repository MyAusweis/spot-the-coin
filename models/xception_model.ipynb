{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xception_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vev5DAYLK5OZ",
        "colab_type": "text"
      },
      "source": [
        "# **Bangkit Final Project: World Coin Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aF2mIOQNC5o",
        "colab_type": "text"
      },
      "source": [
        "# **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu8dZbIdLrLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, LeakyReLU\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.regularizers import Regularizer, l2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0MskslwLsYt",
        "colab_type": "code",
        "outputId": "bf6f106b-f8c9-4521-b922-33bc938b0431",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Upload the kaggle.json file from Kaggle account settings page\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-04870b08-2976-49ea-88ad-bd25567e0c76\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-04870b08-2976-49ea-88ad-bd25567e0c76\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-mZ_XHMezh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the Kaggle API client\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSwixL0MUxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_rjYl-NU0_",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS2VsR0CNg0G",
        "colab_type": "code",
        "outputId": "22b2b2f9-c688-482e-bca0-3ff9bba253ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d wanderdust/coin-images"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coin-images.zip to /content\n",
            " 98% 450M/459M [00:08<00:00, 59.8MB/s]\n",
            "100% 459M/459M [00:09<00:00, 52.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN_bT4N7OopU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the dataset into folder\n",
        "zip_ref = zipfile.ZipFile('/content/coin-images.zip', 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2d5YlhRQpK",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaQrJvGRTG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define directories\n",
        "data_dir = \"/content/coins/data/\"\n",
        "\n",
        "train_dir = data_dir + \"train/\"\n",
        "validation_dir = data_dir + \"validation/\"\n",
        "test_dir = data_dir + \"test/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZoeHe2hJ-SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BDiwC6MJcE",
        "colab_type": "code",
        "outputId": "e56bd446-aa50-45e8-9184-5bb5e9246a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Read images from generators\n",
        "batch_size = 32\n",
        "image_width = 299\n",
        "image_height = 299\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "      validation_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "      test_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6413 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9QD8ft22wk_",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1T4tt0OrhX",
        "colab_type": "code",
        "outputId": "29990654-7693-4fbb-87e0-a0c26cc37b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load base model\n",
        "base_model = Xception(input_shape=(image_width, image_height, 3), weights='imagenet', include_top=False)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "    # Add regularizer\n",
        "    l2_layer = l2(0.01)\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        base_model.add_loss(lambda layer=layer: l2_layer(layer.kernel))\n",
        "\n",
        "for layer in base_model.layers[:10]:\n",
        "\t\tlayer.trainable = False\n",
        "\n",
        "# Custom top classifier for model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(211, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 37, 37, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 19, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 1024) 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          1049088     global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 211)          108243      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,018,811\n",
            "Trainable params: 21,935,771\n",
            "Non-trainable params: 83,040\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnhZ9bR-PiVb",
        "colab_type": "code",
        "outputId": "4914584a-642a-4a53-a8a1-6806e5d097f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Callback to reduce learning rate if no improvement in validation loss for certain number of epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8, verbose=1)\n",
        "\n",
        "# Callback to stop training if no improvement in validation loss for certain number of epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# Callback to save best model weights per epoch\n",
        "weights_filepath = \"best_model_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=weights_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=50,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    validation_steps=3,\n",
        "    callbacks=[reduce_lr, checkpoint]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.3420 - accuracy: 0.0100\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.04167, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 5.3420 - accuracy: 0.0100 - val_loss: 5.3194 - val_accuracy: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.1326 - accuracy: 0.0544\n",
            "Epoch 00002: val_accuracy improved from 0.04167 to 0.08333, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 5.1326 - accuracy: 0.0544 - val_loss: 5.1684 - val_accuracy: 0.0833 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.7339 - accuracy: 0.1369\n",
            "Epoch 00003: val_accuracy improved from 0.08333 to 0.16667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 4.7339 - accuracy: 0.1369 - val_loss: 4.5624 - val_accuracy: 0.1667 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.1974 - accuracy: 0.2250\n",
            "Epoch 00004: val_accuracy improved from 0.16667 to 0.25000, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 4.1974 - accuracy: 0.2250 - val_loss: 4.1060 - val_accuracy: 0.2500 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.6934 - accuracy: 0.2856\n",
            "Epoch 00005: val_accuracy improved from 0.25000 to 0.31250, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 3.6934 - accuracy: 0.2856 - val_loss: 3.6685 - val_accuracy: 0.3125 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.2798 - accuracy: 0.3881\n",
            "Epoch 00006: val_accuracy improved from 0.31250 to 0.46875, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 3.2798 - accuracy: 0.3881 - val_loss: 2.9092 - val_accuracy: 0.4688 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.8298 - accuracy: 0.4494\n",
            "Epoch 00007: val_accuracy improved from 0.46875 to 0.51042, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 2.8298 - accuracy: 0.4494 - val_loss: 2.9898 - val_accuracy: 0.5104 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.5024 - accuracy: 0.5138\n",
            "Epoch 00008: val_accuracy improved from 0.51042 to 0.60417, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 2.5024 - accuracy: 0.5138 - val_loss: 2.3107 - val_accuracy: 0.6042 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.1280 - accuracy: 0.5744\n",
            "Epoch 00009: val_accuracy improved from 0.60417 to 0.66667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 2.1280 - accuracy: 0.5744 - val_loss: 1.8231 - val_accuracy: 0.6667 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.8708 - accuracy: 0.6344\n",
            "Epoch 00010: val_accuracy improved from 0.66667 to 0.67708, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 1.8708 - accuracy: 0.6344 - val_loss: 1.7857 - val_accuracy: 0.6771 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.6873 - accuracy: 0.6603\n",
            "Epoch 00011: val_accuracy improved from 0.67708 to 0.70833, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 1.6873 - accuracy: 0.6603 - val_loss: 1.4786 - val_accuracy: 0.7083 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.4533 - accuracy: 0.6963\n",
            "Epoch 00012: val_accuracy did not improve from 0.70833\n",
            "50/50 [==============================] - 99s 2s/step - loss: 1.4533 - accuracy: 0.6963 - val_loss: 1.5788 - val_accuracy: 0.6771 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.3548 - accuracy: 0.7138\n",
            "Epoch 00013: val_accuracy improved from 0.70833 to 0.79167, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 1.3548 - accuracy: 0.7138 - val_loss: 1.2441 - val_accuracy: 0.7917 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2097 - accuracy: 0.7306\n",
            "Epoch 00014: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.2097 - accuracy: 0.7306 - val_loss: 1.3634 - val_accuracy: 0.6667 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0850 - accuracy: 0.7600\n",
            "Epoch 00015: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 99s 2s/step - loss: 1.0850 - accuracy: 0.7600 - val_loss: 1.0999 - val_accuracy: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9836 - accuracy: 0.7736\n",
            "Epoch 00016: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.9836 - accuracy: 0.7736 - val_loss: 1.0229 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.7894\n",
            "Epoch 00017: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.9483 - accuracy: 0.7894 - val_loss: 0.8715 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8762 - accuracy: 0.8039\n",
            "Epoch 00018: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.8762 - accuracy: 0.8039 - val_loss: 0.9599 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8562 - accuracy: 0.7887\n",
            "Epoch 00019: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.8562 - accuracy: 0.7887 - val_loss: 1.0284 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.8406\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.7079 - accuracy: 0.8406 - val_loss: 0.9714 - val_accuracy: 0.7396 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7036 - accuracy: 0.8319\n",
            "Epoch 00021: val_accuracy did not improve from 0.79167\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.7036 - accuracy: 0.8319 - val_loss: 0.9931 - val_accuracy: 0.7604 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.8571\n",
            "Epoch 00022: val_accuracy improved from 0.79167 to 0.91667, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6153 - accuracy: 0.8571 - val_loss: 0.4230 - val_accuracy: 0.9167 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.8756\n",
            "Epoch 00023: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6015 - accuracy: 0.8756 - val_loss: 0.9879 - val_accuracy: 0.7708 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6298 - accuracy: 0.8564\n",
            "Epoch 00024: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6298 - accuracy: 0.8564 - val_loss: 0.7819 - val_accuracy: 0.8021 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5981 - accuracy: 0.8619\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5981 - accuracy: 0.8619 - val_loss: 0.6114 - val_accuracy: 0.8542 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5508 - accuracy: 0.8731\n",
            "Epoch 00026: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5508 - accuracy: 0.8731 - val_loss: 0.9355 - val_accuracy: 0.7708 - lr: 1.0000e-06\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5784 - accuracy: 0.8612\n",
            "Epoch 00027: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5784 - accuracy: 0.8612 - val_loss: 0.9627 - val_accuracy: 0.7812 - lr: 1.0000e-06\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5628 - accuracy: 0.8819\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5628 - accuracy: 0.8819 - val_loss: 0.9652 - val_accuracy: 0.7917 - lr: 1.0000e-06\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8712\n",
            "Epoch 00029: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5673 - accuracy: 0.8712 - val_loss: 1.0876 - val_accuracy: 0.7812 - lr: 1.0000e-07\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5882 - accuracy: 0.8725\n",
            "Epoch 00030: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.5882 - accuracy: 0.8725 - val_loss: 0.9389 - val_accuracy: 0.7708 - lr: 1.0000e-07\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.8637\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6022 - accuracy: 0.8637 - val_loss: 0.6480 - val_accuracy: 0.8333 - lr: 1.0000e-07\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5832 - accuracy: 0.8687\n",
            "Epoch 00032: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5832 - accuracy: 0.8687 - val_loss: 0.5979 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5517 - accuracy: 0.8731\n",
            "Epoch 00033: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5517 - accuracy: 0.8731 - val_loss: 0.8475 - val_accuracy: 0.7188 - lr: 1.0000e-08\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6259 - accuracy: 0.8650\n",
            "Epoch 00034: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6259 - accuracy: 0.8650 - val_loss: 0.7106 - val_accuracy: 0.8750 - lr: 1.0000e-08\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.8608\n",
            "Epoch 00035: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6179 - accuracy: 0.8608 - val_loss: 1.0181 - val_accuracy: 0.7396 - lr: 1.0000e-08\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.8662\n",
            "Epoch 00036: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5874 - accuracy: 0.8662 - val_loss: 0.6953 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.8700\n",
            "Epoch 00037: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5836 - accuracy: 0.8700 - val_loss: 0.8850 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5632 - accuracy: 0.8741\n",
            "Epoch 00038: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5632 - accuracy: 0.8741 - val_loss: 0.6648 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5876 - accuracy: 0.8710\n",
            "Epoch 00039: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.5876 - accuracy: 0.8710 - val_loss: 0.5828 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.8769\n",
            "Epoch 00040: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5578 - accuracy: 0.8769 - val_loss: 0.8989 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5688 - accuracy: 0.8781\n",
            "Epoch 00041: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5688 - accuracy: 0.8781 - val_loss: 0.7687 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.8769\n",
            "Epoch 00042: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5881 - accuracy: 0.8769 - val_loss: 0.6670 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.8544\n",
            "Epoch 00043: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6104 - accuracy: 0.8544 - val_loss: 0.5435 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.8763\n",
            "Epoch 00044: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5719 - accuracy: 0.8763 - val_loss: 1.0463 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.8675\n",
            "Epoch 00045: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5920 - accuracy: 0.8675 - val_loss: 0.9353 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.8606\n",
            "Epoch 00046: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6203 - accuracy: 0.8606 - val_loss: 0.6945 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5912 - accuracy: 0.8672\n",
            "Epoch 00047: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5912 - accuracy: 0.8672 - val_loss: 0.5778 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5781 - accuracy: 0.8731\n",
            "Epoch 00048: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5781 - accuracy: 0.8731 - val_loss: 0.9086 - val_accuracy: 0.7500 - lr: 1.0000e-08\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.8625\n",
            "Epoch 00049: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6125 - accuracy: 0.8625 - val_loss: 0.7372 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5439 - accuracy: 0.8825\n",
            "Epoch 00050: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5439 - accuracy: 0.8825 - val_loss: 0.7003 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5279 - accuracy: 0.8750\n",
            "Epoch 00051: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5279 - accuracy: 0.8750 - val_loss: 1.0037 - val_accuracy: 0.7396 - lr: 1.0000e-08\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5225 - accuracy: 0.8836\n",
            "Epoch 00052: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5225 - accuracy: 0.8836 - val_loss: 0.9104 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5698 - accuracy: 0.8750\n",
            "Epoch 00053: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5698 - accuracy: 0.8750 - val_loss: 0.8409 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5624 - accuracy: 0.8756\n",
            "Epoch 00054: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5624 - accuracy: 0.8756 - val_loss: 0.5549 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.8602\n",
            "Epoch 00055: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.6280 - accuracy: 0.8602 - val_loss: 0.9160 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.8767\n",
            "Epoch 00056: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5724 - accuracy: 0.8767 - val_loss: 0.9400 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.8694\n",
            "Epoch 00057: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5659 - accuracy: 0.8694 - val_loss: 0.9604 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5701 - accuracy: 0.8781\n",
            "Epoch 00058: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5701 - accuracy: 0.8781 - val_loss: 1.0385 - val_accuracy: 0.7708 - lr: 1.0000e-08\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5472 - accuracy: 0.8750\n",
            "Epoch 00059: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5472 - accuracy: 0.8750 - val_loss: 1.0039 - val_accuracy: 0.7708 - lr: 1.0000e-08\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5838 - accuracy: 0.8779\n",
            "Epoch 00060: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5838 - accuracy: 0.8779 - val_loss: 0.8424 - val_accuracy: 0.7396 - lr: 1.0000e-08\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.8706\n",
            "Epoch 00061: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.5776 - accuracy: 0.8706 - val_loss: 0.5948 - val_accuracy: 0.8750 - lr: 1.0000e-08\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.8737\n",
            "Epoch 00062: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5741 - accuracy: 0.8737 - val_loss: 0.6317 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.8725\n",
            "Epoch 00063: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5693 - accuracy: 0.8725 - val_loss: 0.9361 - val_accuracy: 0.7396 - lr: 1.0000e-08\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.8531\n",
            "Epoch 00064: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6281 - accuracy: 0.8531 - val_loss: 1.0718 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5884 - accuracy: 0.8694\n",
            "Epoch 00065: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5884 - accuracy: 0.8694 - val_loss: 0.7453 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6251 - accuracy: 0.8606\n",
            "Epoch 00066: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6251 - accuracy: 0.8606 - val_loss: 0.9045 - val_accuracy: 0.7708 - lr: 1.0000e-08\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.8731\n",
            "Epoch 00067: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6033 - accuracy: 0.8731 - val_loss: 0.6132 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6068 - accuracy: 0.8619\n",
            "Epoch 00068: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.6068 - accuracy: 0.8619 - val_loss: 0.5641 - val_accuracy: 0.8542 - lr: 1.0000e-08\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6188 - accuracy: 0.8531\n",
            "Epoch 00069: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.6188 - accuracy: 0.8531 - val_loss: 0.9701 - val_accuracy: 0.7292 - lr: 1.0000e-08\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5667 - accuracy: 0.8798\n",
            "Epoch 00070: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5667 - accuracy: 0.8798 - val_loss: 0.6273 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6052 - accuracy: 0.8687\n",
            "Epoch 00071: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6052 - accuracy: 0.8687 - val_loss: 0.9294 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5495 - accuracy: 0.8744\n",
            "Epoch 00072: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5495 - accuracy: 0.8744 - val_loss: 0.8140 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5427 - accuracy: 0.8831\n",
            "Epoch 00073: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5427 - accuracy: 0.8831 - val_loss: 1.0289 - val_accuracy: 0.7500 - lr: 1.0000e-08\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.8519\n",
            "Epoch 00074: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6153 - accuracy: 0.8519 - val_loss: 0.6501 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5644 - accuracy: 0.8700\n",
            "Epoch 00075: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.5644 - accuracy: 0.8700 - val_loss: 0.8204 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6053 - accuracy: 0.8562\n",
            "Epoch 00076: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6053 - accuracy: 0.8562 - val_loss: 0.8607 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8725\n",
            "Epoch 00077: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5772 - accuracy: 0.8725 - val_loss: 0.7452 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5841 - accuracy: 0.8687\n",
            "Epoch 00078: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5841 - accuracy: 0.8687 - val_loss: 0.8673 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.8600\n",
            "Epoch 00079: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6283 - accuracy: 0.8600 - val_loss: 0.8085 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.8806\n",
            "Epoch 00080: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5606 - accuracy: 0.8806 - val_loss: 0.6497 - val_accuracy: 0.8542 - lr: 1.0000e-08\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.8644\n",
            "Epoch 00081: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6078 - accuracy: 0.8644 - val_loss: 0.8501 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5856 - accuracy: 0.8562\n",
            "Epoch 00082: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5856 - accuracy: 0.8562 - val_loss: 0.6089 - val_accuracy: 0.8646 - lr: 1.0000e-08\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.8697\n",
            "Epoch 00083: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5984 - accuracy: 0.8697 - val_loss: 0.9128 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.8863\n",
            "Epoch 00084: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5465 - accuracy: 0.8863 - val_loss: 0.8248 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.8590\n",
            "Epoch 00085: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.5937 - accuracy: 0.8590 - val_loss: 0.8691 - val_accuracy: 0.7708 - lr: 1.0000e-08\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5566 - accuracy: 0.8737\n",
            "Epoch 00086: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5566 - accuracy: 0.8737 - val_loss: 0.7718 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6223 - accuracy: 0.8525\n",
            "Epoch 00087: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.6223 - accuracy: 0.8525 - val_loss: 0.7167 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.8662\n",
            "Epoch 00088: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5822 - accuracy: 0.8662 - val_loss: 0.9914 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.8681\n",
            "Epoch 00089: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6126 - accuracy: 0.8681 - val_loss: 0.8015 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6043 - accuracy: 0.8587\n",
            "Epoch 00090: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.6043 - accuracy: 0.8587 - val_loss: 0.8508 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6167 - accuracy: 0.8678\n",
            "Epoch 00091: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.6167 - accuracy: 0.8678 - val_loss: 0.6007 - val_accuracy: 0.8750 - lr: 1.0000e-08\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.8798\n",
            "Epoch 00092: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.5823 - accuracy: 0.8798 - val_loss: 0.6957 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5822 - accuracy: 0.8729\n",
            "Epoch 00093: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.5822 - accuracy: 0.8729 - val_loss: 0.6403 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5836 - accuracy: 0.8741\n",
            "Epoch 00094: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5836 - accuracy: 0.8741 - val_loss: 0.8712 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6338 - accuracy: 0.8612\n",
            "Epoch 00095: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6338 - accuracy: 0.8612 - val_loss: 0.8871 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5778 - accuracy: 0.8731\n",
            "Epoch 00096: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5778 - accuracy: 0.8731 - val_loss: 0.8625 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.8786\n",
            "Epoch 00097: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5491 - accuracy: 0.8786 - val_loss: 0.8132 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5964 - accuracy: 0.8662\n",
            "Epoch 00098: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.5964 - accuracy: 0.8662 - val_loss: 0.6900 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6467 - accuracy: 0.8438\n",
            "Epoch 00099: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.6467 - accuracy: 0.8438 - val_loss: 0.9869 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.8659\n",
            "Epoch 00100: val_accuracy did not improve from 0.91667\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.5881 - accuracy: 0.8659 - val_loss: 0.7016 - val_accuracy: 0.8333 - lr: 1.0000e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIvbXL32hN48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model weights\n",
        "model.load_weights(weights_filepath)\n",
        "\n",
        "# Save model\n",
        "model.save(\"xception_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZTPuGY8rWm",
        "colab_type": "code",
        "outputId": "abefb275-b37e-42ca-ae9e-4942068c70ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# Visualise accuracy history\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Visualise loss history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOydd3hUVfr4P2fSe0IKpJFA6L2E0AWsKIpg7669LLZ12VXXXcuWn/t1197bWlGxoCgoRUBRaugtQALpCem9J+f3x5mbzCSTMIQMaefzPHkmc+t779x73vO2c4SUEo1Go9H0XkydLYBGo9FoOhetCDQajaaXoxWBRqPR9HK0ItBoNJpejlYEGo1G08vRikCj0Wh6OVoRaHoVQoj3hRD/sHPbZCHEuY6WSaPpbLQi0Gg0ml6OVgQaTTdECOHc2TJoeg5aEWi6HGaXzGIhxF4hRLkQ4l0hRF8hxA9CiFIhxFohRIDF9vOFEAeEEEVCiA1CiOEW68YLIXaa9/sccG92rouFELvN+24SQoyxU8Z5QohdQogSIUSaEOLJZutnmI9XZF7/O/NyDyHEf4UQKUKIYiHEr+Zls4UQ6Tbuw7nm/58UQnwphPhYCFEC/E4IESeE2Gw+R5YQ4hUhhKvF/iOFEGuEEAVCiBNCiMeEEP2EEBVCiECL7SYIIXKFEC72XLum56EVgaarcjlwHjAEuAT4AXgMCEY9t/cDCCGGAJ8CD5rXrQS+E0K4mhvFb4CPgD7AF+bjYt53PPAecBcQCLwJLBdCuNkhXzlwE+APzAPuEUIsMB83yizvy2aZxgG7zfv9B5gITDPL9Cegwc57cinwpfmcnwD1wENAEDAVOAe41yyDD7AW+BEIAwYBP0kps4ENwFUWx70R+ExKWWunHJoehlYEmq7Ky1LKE1LKDGAjsFVKuUtKWQUsA8abt7saWCGlXGNuyP4DeKAa2imAC/CClLJWSvklsN3iHHcCb0opt0op66WUHwDV5v3aREq5QUq5T0rZIKXci1JGs8yrrwPWSik/NZ83X0q5WwhhAm4FHpBSZpjPuUlKWW3nPdkspfzGfM5KKeUOKeUWKWWdlDIZpcgMGS4GsqWU/5VSVkkpS6WUW83rPgBuABBCOAHXopSlppeiFYGmq3LC4v9KG9+9zf+HASnGCillA5AGhJvXZUjrkRVTLP6PAh42u1aKhBBFQKR5vzYRQkwWQqw3u1SKgbtRPXPMx0iysVsQyjVla509pDWTYYgQ4nshRLbZXfQvO2QA+BYYIYQYgLK6iqWU29opk6YHoBWBpruTiWrQARBCCFQjmAFkAeHmZQb9Lf5PA/4ppfS3+POUUn5qx3mXAMuBSCmlH/AGYJwnDYixsU8eUNXKunLA0+I6nFBuJUuaDxX8OpAADJZS+qJcZ5YyDLQluNmqWoqyCm5EWwO9Hq0INN2dpcA8IcQ55mDnwyj3ziZgM1AH3C+EcBFCXAbEWez7NnC3uXcvhBBe5iCwjx3n9QEKpJRVQog4lDvI4BPgXCHEVUIIZyFEoBBinNlaeQ94TggRJoRwEkJMNcckjgDu5vO7AI8DJ4tV+AAlQJkQYhhwj8W674FQIcSDQgg3IYSPEGKyxfoPgd8B89GKoNejFYGmWyOlPIzq2b6M6nFfAlwipayRUtYAl6EavAJUPOFri33jgTuAV4BCING8rT3cCzwthCgF/oZSSMZxU4GLUEqpABUoHmte/UdgHypWUQD8GzBJKYvNx3wHZc2UA1ZZRDb4I0oBlaKU2ucWMpSi3D6XANnAUWCOxfrfUEHqnVJKS3eZphci9MQ0Gk3vRAixDlgipXyns2XRdC5aEWg0vRAhxCRgDSrGUdrZ8mg6F+0a0mh6GUKID1A1Bg9qJaABbRFoNBpNr0dbBBqNRtPL6XYDVwUFBcno6OjOFkOj0Wi6FTt27MiTUjavTQG6oSKIjo4mPj6+s8XQaDSaboUQotU0Ye0a0mg0ml6OVgQajUbTy9GKQKPRaHo53S5GYIva2lrS09OpqqrqbFEciru7OxEREbi46PlDNBpNx9EjFEF6ejo+Pj5ER0djPdBkz0FKSX5+Punp6QwYMKCzxdFoND2IHuEaqqqqIjAwsMcqAQAhBIGBgT3e6tFoNGeeHqEIgB6tBAx6wzVqNJozT49RBBo7SFoH+e2dHEuj0fRUtCLoAIqKinjttddOeb+LLrqIoqIiB0jUCl/dAb/858ydT6PRdAu0IugAWlMEdXV1be63cuVK/P39HSWWNfW1UJEHpZln5nwajabb0COyhjqbRx55hKSkJMaNG4eLiwvu7u4EBASQkJDAkSNHWLBgAWlpaVRVVfHAAw9w5513Ak3DZZSVlXHhhRcyY8YMNm3aRHh4ON9++y0eHh4dJ2R5rvosPdH2dhqNptfR4xTBU98d4GBmSYcec0SYL09cMrLV9c888wz79+9n9+7dbNiwgXnz5rF///7GNM/33nuPPn36UFlZyaRJk7j88ssJDAy0OsbRo0f59NNPefvtt7nqqqv46quvuOGGGzruIspyzJ9aEWg0Gmu0a8gBxMXFWeX6v/TSS4wdO5YpU6aQlpbG0aNHW+wzYMAAxo0bB8DEiRNJTk7uWKEMi6CyAOpqOvbYGo2mW9PjLIK2eu5nCi8vr8b/N2zYwNq1a9m8eTOenp7Mnj3bZi2Am5tb4/9OTk5UVlZ2rFCGRQDKKvCP7NjjazSabou2CDoAHx8fSkttz/hXXFxMQEAAnp6eJCQksGXLljMsnZnyZopA02E0NEhszfSXXVxFUm5ZJ0jUdcgprbJ5b84k8ckFXPPWZtIKKjpVjq6MVgQdQGBgINOnT2fUqFEsXrzYat3cuXOpq6tj+PDhPPLII0yZMqVzhCzLtfj/9BTB8j2ZzHtpI/HJBacplGOoqKkjv6ya+gbHN0BVtfUseO03znnuZ1buy0JKSU1dA69vSGLOfzZw3nM/8+TyA5RU1Z7WeSpr6m02qGfiGm1RUVPHVzvSqaqtt7m+oLyGxV/sIe6fP/HBpuQOPfe24wW8+XOSXQomJb+cOz6MZ8uxAp5fc6RD5bBkZ2ohP+7PatfvUd8gO11Jdbs5i2NjY2XziWkOHTrE8OHDO0miM0u7r/Wr2yFhBdRWwLznWO05jwAvVyZF92lzNymlVUVzWkEFc1/4hYraekxC8KcLhnLHzIGYTO2sek7+DUKGg2fbchhkF1dRUVPHgCCvRrn2ZxTz0eYUNh/LJ6+smooa1TgJAX08XRke6sur10/Az6NpsL6ckiqeX3uEqyf1Z1xk+1N4//H9Qd759ThRgZ6k5FcwJsKPypp6juaUcf6IvoT4uvHJ1lSCvd24aWoUUkJ5TT3h/u7cMCXK6t7uSCngtfVJXBvXn3OGhyCEoKiihn//mMCn29II8nZlfP8ARob5klZQyd70IpJyy5gaE8jD5w9lQv1+6DsSPPuQW1qNlJI+Xq44O516f6/5796cf644yNsbjzMq3JfXr59IZB9PAGrrG1i2M4P/98MhSqvqCPJ2wyTg5z/NwaUdcjQnp7SKC57/hcKKWu4/ZzB/OG9Iq9sWV9Sy8PXfKCivYebgYL7fm8nqB89icF+fxm3KquvwdmunhzwnAYSJpckePLZsH3UNkphgL+47ezAXjQ6lpKqWvLJq/DxcCPVrmQGYml/BFzvS+HJHOlnFVdx39iD+cN4Qh40gIITYIaWMtblOK4LuRbuv9YP5UFMGGTtJHb2IWfFT8XRxYsX9M4kO8mqxeXFlLYu/2ENiThlv3DiRIX19aGiQXPv2Fg5klrD0rqm8vO4oP+zPZmJUAD7uzuSVVVNXL7lr1kAWjAu3eqALymv46dAJVh88wZZj+cwaEsyj5w8k/LUYmPlHmPOoTbFr6xvYlVrEhsM5rEvIISFbueD8PV2Y0D+A4spadqQU4u5i4uxhIYT6eRDk7YaHi4mC8hpySqtZGp/GtXH9+efC0YBq5O74cAdrD51ACLh+cn8WXzDMSlFYklZQwcdbU/h2VyaTBvThnwtH4evuwq9H87jh3a3cNDWKJy4ZybJdGbyw9ggmIXjikhGcM7wvAHvTi3j8m/3sTS8GwNkkqGtQ9+nRC9VveTyvnMte+43iyloaJIyN9OfCUf14+5djFFXWclVsJNV19exOLeJYXjlB3q6MifAnKtCT5bszKSmv4JD7LfzmP5/Hqm4io6gpxhTg6cIt0wdw/zmDra6roqaO2jqJn2fTddfUNfDo1/vYejyfl64dz4T+AS3uR1ZxJbOe3cDYCD8SsktxMgmemj+SIydKWRqfTm5pNbFRAfxz4WjSCiq4/cN4XrxmHJeOC288RkJ2CaG+Hlbnrqtv4I9f7CGzqIrpg4KYMTiQsRH+jYpMSsntH8Tza2IeZw0JZs3BEzx7xRiujG0Z76qtb+Dm97axPbmAj2+bzOC+Psz89zpmDw3h1esnAPDdnkwe/Hw3f7loOLfOOPWBHOXb55JVVM60/MeZOTiIKyZG8PqGpMZn1MDVycQL14zjotGhgHIl/mf1YV7bkIQQcNbgYHzcnfl+bxaXjQ/nmcvH4Orc8c6athRBjwsWa1qhLAcCY6j3DCJ+/yEGBJ1LflkNiz7dyVf3TMPN2alx06MnSrnzox2kFVTg6+HC5a9t4uXrxpOUW87W4wX8+/LRjAjz5bXrJ/DBpmQ+2JxCTV0Dwd5u5JRW89Dne/giPp3HLhrOkROlfLM7kz2JqRQ3uBPq58Es80u8/+ABNjjXUZ2fjJuFqEUVNXy3N4v1CTlsPZZPeU09TiZBbFQAj1w4DH8PF3amFrInJQ8X6nl83nCunBhp1ahY4uXmzLu/HueyCeFM7OvEiiPlrD10ggfOGUxxZS0fbk7mx/0nuO/sQVw9KRJ3F3UvticX8ObPx/gp4QQmIZg6MJCV+7LYm17EvxaO5uEvdhMT7MWjFw7HySS4YmIEV0yMaHH+MRH+fPv76ZRW1+Hh4oSzSfDXb/fz5s/HCPRy5cqJkdz6/naEEKx+aBY7Ugp46adEnvkhgXGR/ny4cBQjw/waj1dZU4+7i6lR0f7x/KF8tW4zzlvrGVC8hXGD7ueW6dG4OZvIK6thZ2ohz605wsgw30blVFxZyxWvbyKzqJKHzx/KzdOiKa+p4+6PdrApKZ8gb1eufnMzT186imvj+ltdz0s/HUVKyfMLBlHn7MVdH+3ggc92YxIwZ2gI18b15+xhIZhMgsEh3gwM9uLtjceYPzYMIQTbjhdw7dtbiOrjyZI7ptDPzx0pJX/99gDf7M5kSF9vXvjpCM+vhahAT/5+6SjOGhLMF/Hp/JSQw18vHsFNU6O45X/befTrfYT5ezB9UBBICTVlVJk8ue/TXWxKyue/V45l8kCVqn3bjAG8tC6RezOLySyq4qHPd+NkEvz7xwRmDw1mYLA3oBTOZ9vT2JFSSGpBBRmFlUyNCeTxecPx93QFlGXqlXWEkPpSbo4N5vGFsbg4mbhkTBhrD51gf0Yxgd5uBHq78sGmZH6/ZCdPXzqKyyeE84fP9/DjgWyuio3gwXOHEObvgZSSYf18+M/qI2QVV/HOzbF4WVgqUkpeWZfIlbGR9PNzt+eNPyW0RdDNaPe1/t9AGoZdQureXzhWG0Dkvd9yLK+cuz7awW0zBvDXi0dQWlXLN7syeOaHBDxcnXjt+olEBHhw+wfxJGSX4GwyMWNwEO/eHNuq+drQIFmyLZV//5hAaZWqrJ7ld4L3ahaTNvd9ouIuRghBRlElS77+hsWpd7OxYTQvhf0fs4YEc/hEGasOZFNT18CAIC9mDApi+qAgpsYEtuyxf/8QpG6BezYpP1ArlFfXcd5zPzPPtIXHqp/jUvEi0j+aZfdOw9nJxL70Yp767gDxKYWE+LhxbVx/fkvMIz6lkABPF66fHMX1U/oT6udBfHIBi5bsIrukChcnwbJ7pzMq3K/Vc7dGfYPk/k93sWJfFgODvEgvrGTJHZOJNbvqquvqOXqijOGhvjjZ43ZL3Qrvna/+f3Af+Dc13lW19Sx8bRMnSqr44YGZBHi6cvN724hPKWBC/wC2Hi9gVLgvdfWSxJwy/u+KMZw9LIT7Pt3FxqN5XBsXyV/mjcDbzZljuWWc9/wvPD6ygFuSHoRF26nwjmRdQg4T+gcQ5t/SBbJkayqPLdvHp3dMISbYi3kv/4q7i4nC8lr6eLmy5I7JfL83i2d+SOCe2TH8ee4wCstr2JiYxwtrjnAsr5wLR/Vj49E8RoX7suT2KZhMgpKqWq58fTPJ+eXcPSuG3/fZjsuqP3FX8EesTqrkqfkjuXladKMcxZW1zPz3OsIDPEnKKWN4qA/PXz2Oha9tYnCIN5/fNRUBPLH8AB9tSSHEx43oQC/6eLmy9tAJArxc+eeCUeSV1fDCyl1sEzcBIG/+HjFgZqs/TWVNPYuW7OSnhBzC/T3IKq7kL/NGcOv0lsPmL9uVzh+W7uGWaQP42yUjrJY/9Pkenr50JDdNjaY9aIugt1NfBxUFbD7hRG21D+MDqwjo68Pgvj7cPDWKd389TlpBBRuP5lFZW8/EqABeuW58o1/zi7un8oelu9mdVsQzl41u04dpMglumBLF+SP7snx3JuMi/Zm488+IvXVEN6Q1Ntjh/h4snh4AqTDcq4yKmnr+s/oIfh4uXBfXnysmRrTdwEqpYh5lJ6DgGATGtLqpl5szT88fSfjnDyJMdcTUHeKOy+c3uhxGR/jxxd1T2Xwsn5d/SuTFn44SEeDBU/NHclVsJB6uTdZSbHQfVtw/g3+uOMTkgX3apQQAnEyC564eS3FlLb8m5vHiNeMalQCAm7PTqR3bcuiQpPUw8ebGr+4uTrx87XgueflXHvp8N/383Nl8LJ/nrx7LgnHhrNyXzVPfHaCipp7/3TKJmYODAXj/ljieXXWYN39JYsPhXJ64ZCTf7c3EzdnEFRHFcLQW0rbhOXYAF48Ja1W0yyaE89/Vh3n95yQqa+ooq6rj49umU1lbz03vbmXBq7+RV1bDJWPDWHz+UAACvFyZPzaMC0b25Y0Nx3h1QyKuTiaevWJsYzzK192Fj26P4+nvDvLiT0cZ4LmMBQ1lFB7fzbNXXNnCZeTn4cJds2J4dtVhhvXz4YNb4/D3dOXJ+SN46PM9/O+34yTmlPHZ9jTuOmsgj1w4zCoOtfjLvdz50Q4AruxfBeZEPJG+HdpQBB6uTrxx40T+smwfP+zL5p2bYzl7WF+b2y4cH0F8ciHvbzrOwvHhjI7wo6C8hr9/f4hxkf5cPzmq1fOcDloR9AYq8gDJquQGLgkOJ6B+d+OqRy8azo7UQn5LzGPB+DDu9VhNhMhH+E1r3MbLzZk3b4ylrr7B7sBjiI87t88cCMUZsP9rtbAs23ojc0prkCxgxf0zyS+rxtvd2cpN1So5B5uyn46tb1MRAJzrfghMqQBcF1nAiDBfq/VCCKbFBDEtJoiMokr6+ri1eq2B3m48d/W4k8t4EtycnXjn5lhS8isY2s/n5Du0Ran53rr5qVFmLRQBwKAQb56cP4I/f7UPgD+cN4SF45Uba96YUOYMC6a8up5gH4t6FpPgkYHJ3JP+JteWPcTdH6tGcNGcQfjUbFYbZe2BsVe3KZq7ixM3To3ihbWqkPLFa8Y1Xu+SO6Zw47tbiYvuw7NXjGmRdODm7MQD5w7msgnhVNfVq6D01rcgbStc8S4hPu68ct0EbppagPcnf4cG+GucYIyNuAEo95Crk4kF48Mb3TwLxoXz/Z4s/rHiEIDNoO2ocD++/f103t90nEAvNy7zOQhLAOEEadvavH4AFycT/3fFWP61cPRJ36E/zR3G6oMneHTZXr65dzr/+P4gJZW1PHP5aPusw3agFUFvwFxMVuYSwKihvrB9NTQ0gMmEu4sTX90zDSnB3dkEL14PlUVwwb9auFvak33CtjdBNoCbb8txjowit6piqKkg0NvT/uMmrVefHgHq/0m3t739ppdp8AqhUPgzwS29zU3Dbbg3HIW7i9PpKwGAkkxwcoNh8+DID9BQDyZrhXpVbCRHT5Th5CS47+xBVus8XZ3xdG3WHEgJa5/ALzeBbx94jf/tq+bXxHzuOGsg/GBWPNl77RLvxilRfLQ5hQXjw62CxqPC/dj457Nxdza1+XwZWUlk7YFVj0JDHcz7L3iojK+46ACkUwbUwRjXrFaP4+7ipOS3QAjBvy4bzXVvb+GyCRH8fs4gm/u6Opu48yxzh2P7j+pz4CxI36bulR3ZPva8Q34eLjxxyQgWLdnFA5/tZsW+LBbNGcSwfr4n3be96DqCDqC9w1ADvPDCC1RUODaHeO9h1RObPWE0Hn3C1UtUkd+43s3ZSQVIC45BUSpUl0BJxumfuLoU4t+HEZdC0JCW9QvlFrUNpa2/vDZJWqeOOXw+HP9Fub9a48RBSPoJ0+S7CBw6HafsPerFtUf+jc+pe9LVKc0Cn34QczZUFqoGsxlCCB6/eASPXjjcvhTFxLWQmwCAS3k2d54Vw4e3xqlYjeGKytpr170M9HZj06Nn89eLR7RY5+3mbF8no64alt3TdL7sfU3rSrMQ1eYxxnIPnfxYzejr685PD8+2rQR2fQKZu6yXFaWByUU9fxX56t3pQOaNDmXO0ODGGNKis20rp45CK4IOoCspguq6eq56YzNPLj9AaVUt9Q2SVVvVC3PBlNHgbfZN2ioqS1rX9H9OwukLs/MjqC6GqfepRqr5OS2HvTgVRVBbBSmbYOAciJmjFFfmzta33/wKuHhC7K0QOkZZICdr3LP2wpuz4KenIP49+2XrLEqzwTcMBs5W34+tP/1jbnoZnJT7hOJmVlRJFgiT+n0Lk+06nF0uv7b4+d+QcwAueUF9t7RGcg6qz4BoyDl1RdAqNRXw3f3w24vWy4vTwC8C+psLRNO2dtw5UUr77wtGMWVgH569cmxjJpuj0IqgA7Achnrx4sU8++yzTJo0iTFjxvDEE08AUF5ezrx58xg7diyjRo3i888/56WXXiIzM5M5c+YwZ86cDpFlR3Ih25ILeH9TMuc+9zOPfb2P2hJlxrv59VMNMrT01wMc2wCeQer/dvSqrKivgy2vQ/9pEDFRKaDS5jGCXPA0j8JacgqKIG0r1FUqJTBgFiCslZglJVmwdymMv0EVrYWOVctt9Jgb2f4OvHOOKr7zCob8xLblyU/q/OG9SzLVb+sdDH1HN7nO2twnq/VGPGsvHP8Zptxr3raZhViaDZGTzdu2cS8tydylGtb2kB4Pvz4P426ACTeBT6j1eY2Oy8iF6rkqb7J4kVJlVbUnQzJzl7Kgm3eMilJVZlbQUBWX6WBFABAR4Mlnd05lYpS5lmPXx8pKdQA9L0bwwyPWJmNH0G80XPhMq6sth6FevXo1X375Jdu2bUNKyfz58/nll1/Izc0lLCyMFStWAGoMIj8/P5577jnWr19PUFBQh4i6MTEPZ5Pgw1vj+MeKQ3wen8ZLfaqRNR4IV2/wDlEbNm+46uuUi2XkQjjy4+n3qlJ+heJUuOCf6rtPv6aRT53NvcyyHNUwJ607tQlzjq0HkzNEzwA3Hwgbpxq+2Y9Yb5e1B764Rf0/5R71GTJSBfiy9sCI+S2PnXsEVjwMMefAZW/B8vsgrxVF0NAAm16En/4OQy+Eaz6x/xo6EimVRTVkrvoeM0cp4ZpycG1ZLNjI8vtUA3/v5pbrNr8Crt4w4yHY+qYK+htUl0JNqXJDpW9XPfORC9qWsaoE3jkXJv5O+fZPlXX/UJ2Juf9S3/uNUcrKIPeQUtpRM5TCyD0EXjPUukPLYelN8LuVED391M6bbg4E5x+1fnaL0mDwuWAyQeQkuwLGp8Wh7+Hb3ys31PQHOvzw2iLoYFavXs3q1asZP348EyZMICEhgaNHjzJ69GjWrFnDn//8ZzZu3IifX/vSDk/GxqO5TOgfwLRBQSxfNJ0XrxnHuf0FwjtYBbO8W7EIMnYoF0vMHDXkw+kqgtStgGhyVRgKyHLwu/Ic6DNQNTjNrQUpVUNri6R1EBGnlAAoF1H6dtXYGPtufVM1PLWVcNO36jwALu4QPLT1IGfST+rz4ufBKwgCByn/b0OzMXXKcuGTK2Dtk+DsBicOtH0/HEl1ibJefFXlKjFzoKFWuc9aQ0rIiFculYpmY0YVZ8D+r1TP28Mf/MKhxMI1ZFhvAdEQPNw+iyA3QfWsd33S8nwnoyxXWSfjrgN383sTOhbyDjdZGDkJ6rkNMdfYWD6/R9eoz9Q27kdrGA18Qx0UmOf7rq1S74+/OZUzcrI6X5WqHEdKFacx/uwZ9r26rGn7mnLrdeX58P2DSvlNvufUr8EOep5F0EbP/UwgpeTRRx/lrrvuarFu586drFy5kscff5xzzjmHv/3tbx167vyyavZnlPCwefwVZyeTytDYmw9e5obY1VNl8Fj658HsUxbK1ZK6FXZ+2JhZ1C7StkLICHA3ZzoYCqj0hPKt1tWoh94rRFkLJc0sgvcvhqDBTf5gg/J81ROc81jTspiz4dfnIHkj9J+qeroJ36se8qWvgZf1JECNVogtktZDnxgIML/kgYOgvlr5hAOim7b75HLV+Fz8vJJ943+te4xnEqNh9jErgv5TVQbRsQ0w+Dzb+xSnq/sPSokOuaBp3Y7/qUyvyXer777h1haBEc/x6adiLkdXnzxrxvDh11XC9ndh1uLWt23OoeVKnpGXNS0LHaOW5RyE8IlK0Yy7XsVJ3HybFIGU6j7AqffapVTPcd/RcGKfOmbI8CY3mZ85RTUyDpDKfRU0BL6+A1ItrCz//nD/ntbfpWMb4MMF6higYi8zHoLZj6nMrxUPqUy+m7512POlLYIOwHIY6gsuuID33nuPsjI1/HBGRgY5OTlkZmbi6enJDTfcwOLFi9m5c2eLfU+X35KUX3TmkGDrFWW5TT1ysO2vT1oPYeOVHz1kGNSWK9dOe2hoUC9FZFzTMh8jSG0+r5Ex5B2sGjDLYHFdtXqRdn1k3QABHN8ASNX4G0TGqWDwtrfhzbPgyCo4/59w7WctlQConlXZiZb3oK4Gkn9VPWqDIPP4PJZxAiMrZ9afVAA6cLBqlAqPn+zOOAbDrWYoAhcP6DsCTuxvfR/LXnxz//bR1RA5pW9b9TcAACAASURBVEkZ+kVYK+pGRRCmlGp5bst72ZycBPUbDTpXpRTXtpyTo1UOLFMNbF+LuUYaYz27lZKuKVPPrRAQPKwx24n8JLXe1UcpgtasTFsUHFOumPE3qMbZUC5FKerTqN4On6jWb34F3pgB2fthzl9g7jNKORWlKtdSa+z6RFk6c59Rf6OvVB2L9+fBltfg4LdqLK6+jptrRSuCDsByGOo1a9Zw3XXXMXXqVEaPHs0VV1xBaWkp+/btIy4ujnHjxvHUU0/x+OOPA3DnnXcyd+7cDgkWbzySi5+HC6ObV6SW5yj/qYF3X+sMnqoS1Ss0GtcQc4pfezOH8g6rbBJLRdBoERiKwGyReIWoXpylIshPBFmvzPGtb1gf+8gq9dKEjW9a5uwGUdOVVSNMcNsqmLao9R5qYyPSzD2Uvk0pQEslE2hO27OMExgxKEOGoEFNctvD4R/hrdkqK+nNWfDpdVB/GsNUG/fUcA2Bctm09ftl71X3KmiodU/ZsLgslaFvuFLgRoquoRR8+imlCid3D+UeUg30tPuV4ti31M5rO6GU88iF1r+nXyS4+ytZjesMNruFDNemlE2W3+Q7oarI+jeqLoPPrlcNty2M+zLgLOVaNBIoitLUpzG5k5uPij0lrVPK866fVSdhyj2qZw+tB5NrK+HwShh+idp+yj0qNnXZO0qRr3oMwmNhWsfHBSxxqGtICDEXeBFwAt6RUj7TbH1/4APA37zNI1LKlY6UyVEsWbLE6vsDD1j/cDExMVxwwQU057777uO+++477fNLKfk1MY/pgwKtqw8b6lWvxkgbBdU7z7BIt0zeqBpe4+UPVmX+5B6CoXNPXRjjoTeySsCsiESTAjLmR/A2u4ZKs5vcC0bPK2Qk7HgfzlqsXEzJv6kMoMl3tSiWYubDqkd41uImP3Jr9FOjkJK9B4ac37Q8ab0KJEfPsJbbzc+6ATEUiKFQGpVFG70+g4YGWPM35U8OHausi8MrlAUVNfXk+9uipJlFAOpe7Fmi/PG2hvjO2qN62QNmKcurvg6cnG1bXL5hyuIpzVKNX2m2cr+4eUO/UYBQiqWtZyXnkLIGBpyl7v+mV1QG0Mlcjwe/VfJYuoVAPSehY9V1GPGfkGHmz+Gw8wPl/jy2Xvnyx1yjetlpWyHYPHR1wvfqT0q41vr9BdS2br5KgQUPa1I4RanqOfGxGFZj9iPKTTX9AdUxMQgcpIoe07aqmEtzEtcqa2bkQuvlY66E8Anw2wtKmTg51ovvMItACOEEvApcCIwArhVCNK8meRxYKqUcD1wDtC8ZvxeyI6WAuS/8wk+HVMOalFtGVnFV4zgxjVTkq5fYyjXUzzpGkLQeXLxUABZUQ+ob3v6Acdo2lRZqvKCgHmSvoCZF0GgRBKsXqr6mqcgtN0G9aPP+qwKhuz5Svbdv7lE9rrP/2vKcUVPh/H+cXAmAUip9BrbsxSatg4hY62MIoYavsDTts/ao++NlzvRy91OWjT0WQeJaZTGd9zRcv1T9CVPLmEVNuRpKwZ5AY2mW6h27WFREG73j3Fasgqy9qiGNjFOBZsONlLSupcXlZx5R1fCNl2Y2KR03H3V/2rIIKgrU7x5sdt1Mu1/dg8Q1Lbc99nOTTx/gwNfKQjUaeUtCx6jGN3ufksfDnGZpBIxP7IPjG1UHx7JBbjz2MvV5eKXtzLC0bep5MJnUMQuSlEurOE39/paN8/CLlRVgqQRAXW/k5NbjE/u/Vu/KgFkt1wXGwPyXrd8jB+FI11AckCilPCalrAE+Ay5tto0EjLppP+AUcgh7L3vSivjde9tJyC7l3k92suVYPr8cyQNgxqBmaahlFg2ugU9f5QKpLlWm6YFl6mWxDESdTuZQ2jb18Dd3zXj3a0pbNeQyLAJocg/lHFIPf9RU5fLZ8roykYtSYcHrqid6ujRPP6woUDnjlj1hg6DBzVxDe5tcIgaBg+xTBJtfVopvlLmH6xEAYRNaFoDt/Ah+WAy7Pz75MY1iMktsZc8YlOWqxrzfmCb3XZp5mISkDarXbmlx+ZqHhDCKykqyrN1Qze9lcwxlZLgcRy5UFurOD623q6+FL26GDy9VaeAFx1WsqHlv2SB0nOpAHFmllIyBoQR3fdyU5moyqY6O0SBXFkLiTzDmanBygS2vWh+7qlgpGcOqDR6mOlT5R5tqCOwlMg7yjrTMlqqpUKnaw+c7vMd/MhypCMKBNIvv6eZlljwJ3CCESAdWAjZ9JEKIO4UQ8UKI+NzcXFubdPq8qO2ioV49yHYipaS2voGb3tuGv5cLK+6fQWQfT27/IJ6l8WkMCPJqGpPFoNyiwTUw3ESlJ2Dv52pQOiNDxCB4mHp4m6dNggrmtuYGKc9XL0vEpJbrfPpaB4tdvFSeu9GIlVgoAqMhm3af6oHt/EAVN0VNa3nc9hA6VgX9jMyZ478AUqWiNidwkEqfrKlQf3lHmtxCBkE2FEFVifUUoVl71Hmm3K0aH4OYOSp9t7KoaZnRW9386skDnEYxmSV+ESpAassiyDb33kPHKl+7T6iKj+Qnqutsrgz9zK9to0WQbe2GCh2rEguOrFLXl7rVWmYjY8jo1Tu5qMb96JqmlF9Q1kBloVJEW1+Ht8y95OZuIQNDGdeUNj0voJ51jwA4uFxZWwPOUssjJylLpLJQjVzbUKvcjGOuht1LoDyv6RgZOwDZpCgt42ZFaU3xAXswLO106+HzObpKWWOtKbozSGcHi68F3pdSRgAXAR8JIVrIJKV8S0oZK6WMDQ4ObnEQd3d38vPzu58yKDuhCpjsQEpJVk4uuzLK8XJ1YsntUxgZ5sdHt6mxXxKyS5k52EZRWplFUNagURFkKV9t6FhrvzioF6uuqmXlae4RePtseCUWVv2lpesifbv6tIwPWJ7X0iIwlJPRqJRmKgul8HjTiz34AtXDCxoC59hwCbUX4wVfdo/qqSWtU/7g8IkttzViAAVJql5ANii3RPNtynOtG/Pl98ELo1WcQ0p1r119VFGVJQPnqGMmb1Tfi9MhbYuyFPITVa+xLUqzrP3VYM6eGWrbIjB67/1Gm10XccplYlQjN1eG7n5K7uIM1cCXNVMEhtJfchV8cImaF2H/l03rcxLUvfW16AeOXKjScg//0LTswNdqu+u/hGuWAELdAyMY35zAGNWZAGtFIIRquGW9cnEZLiPjmUyPV4o2IFodf+oi9axvf7fpGGnb1PnDzcP3Bw5SRYzZe9RzeioWQfgE8yilzQLG+79W72Xzd68TcKQ9kgFYqs0I8zJLbgPmAkgpNwsh3IEgGkf6to+IiAjS09NpzVrospTnqoavQKieSxvUN0h2ZZbz/p5SPrljSmPPP9TPg49vn8yjX+/lKltD7za6YCxdQ+be484PVe/98ndbunGCLVwLxhDPuz9VVbcu7irFbfMrkPIbXPE/6GOe6i9tq3phLH3MBt59lYXS0KA+DUVgKKaSLNXblg1Npr7JBLf+qHqRlj7w0yVqukrVW/1XeGOmsswGnGXbRDcUQX5iU6+xuUUQaKSZJqkhNeprlevB5ATfPaB6v0d+hLi7WsYxIiaporqkdSp75MA3avnCN+Hjy9WYP8Musn0dDfWqQ2HpqjEIGW7d0Bpk7VGNoHnkTiInq6Dsnk/VcuO3tMQvXFkE5bkqm8vSFRU1De5Yr3q3AMvuhn1fwpir1PfchKb4QOM1xynFcGCZGsa6rkZVzw6bp/zsw+bBA7vbHhbC5KSC1Wlbm55Xg+Bh6tm0tG7CzA3y4R9UHGLafWalMQwGnw/b3lLBXhd3c/3AyKY6GGdXVV+SuE49n36nYBG4eimla6kIqstUmu74G1smPnQCjlQE24HBQogBKAVwDXBds21SgXOA94UQwwF34JRbcxcXFwYMOPU5Rzud125Tg2jdu9V2MMxMSVUtV72xGe+C/az2eQmX+q9R8XfFALdSPqu8G6qeB5qZ9eU5qrjIzWIIW6Ph3bcUfCPU6KDNscwcGjhbKYC9n0H0TJXe5humfJvLF6nc/UteVH7v9O3qoXe1MaS0Tz/ViFQWKJeJoWCcXVUMozSrKTMjpOn6GhusjkQIlaoXORm+vFW5RAbOtr2tIWdeonKBePSx7t2ChbI4qhRBerxyWVz5gbIk1pmH2pjSzAUH6vqjZzT1yA8sU26P4CFKxlWPQvoOddzmlOWohsmnFUWw6yOlvLwsrMXmMQ7DdZG5U9VF2MLXrAgsi8kMhFC9XoORC2DLG8oF4xGgXEPD5lkfz2RSVsHWN5UVlbpFpRxbuoGMnnxbhI41K4KhLa8drK0bN2/VuO/8QD2Hli6Zafcpa+ZfoYBQ1sTEW1oe86BZSZ+KRQDqObPMztq3VFkhXcAtBA50DUkp64BFwCrgECo76IAQ4mkhhDHIy8PAHUKIPcCnwO9kt/PvtBMple8brMv3m1FdV8/dH+0gMaeM16M24FKepVLKLNn6pnLhxP+v5QGMYjLL3phHQNOoklPusfZXG7h5q4f9yGrlq923FGY/qqobjd7giPlw10b1En55i+r5Zuyw7RYCi3GOsq0tAmgqKss9pIb3PclEMx1G+AS46xeY95wqHLKFq5dqCPMTVW86dExLCyogWvU2jdhJ0jpl5Q2crVJbb18DV3/cegMSc7ZyiR3/RQ39YASTJ9yo0lc3v2x7v+bFZJYYVpWle6iqWBVKWVo0oWNUZwFsx0hAWQTFGdbFZK0xcqHyvx/6Xj1/Ffkte+yW2yWsUG4hd//WlXFrTH8ArvqwqeduMOZquOQlVWVtSeRkpQT6xFgrw+iZcNF/YMYfVLrmWX9qOaaPpfvpVGIEYJ2dVZwBa55UsjWXr5NwaKjaXBOwstmyv1n8fxA4xVGgeghVRSo1ElpWzxqb1NZzz8dqMvE3LgogeN1q1Yjv/wrOeUK9nNVlEP+uanSOrlaZQMYYPNCywQXzmEN9VaNgK7fZIHi4Cmj5hMLN39n2ZQZEwS0/qEHBDAVlWUhmiVFUVpKh/PJezRRBSaa6jsBBtpWTo3D3hUm3tb1N4CDVs81NaBrAzhJnV3UvjIDxsfUq3mBYM7ZiD5YYDfCKh9Wn0VN081GzjW1+RbmJEOrenP8PpSxtFZMZWGYOGVMpGsVTlorA2U258tK3NQVWm+MboZ6lwpTWz2cQNkEpxgPLmqqTQ2wogvCJSjHu+RQyd8PIS099CAW/iKb0VkvcfVvM0gYoRbD9baVoLZW5EBB3R9vnasxMEup+nApG5yhtm3nioFpY8Fr7h3DpYLqGFL2RIouEKhuTwFTU1HHbB9vZcCSXfy0czdyyZcr3ft1S5QowKm53f6Ia9HOfVKbmkVXWByrLtW5wDWJvgfP/3rInZcnE3ylFcfevbQe0nFzgvKfghq9UAxZzju3tjGEmTuwHpHXcwtdsEVhmDHUlAgcpl0p9TcvU0cZtBitFUFmkLKPWete2CBqsrI68I00NqcHU36uYRkWB6l0nrVNj84NFMZmNHrpPqIpHWA4pnmWRMWTJ5LtUT7g1N5xhBWbsUMra1jNlIIR6Do5tUEWAYPs3NbZL3qjcaGfCTTLoHBh2MUywoSROhnENPqHtU1g+ofDL/6nf77ynz0h9gL1oRdBZWE6M0swiKKuu4+b3trE5KZ//XDGW60Z7q5zoMVep3vaIBSoTpbJQpRdGTjZP/hLaND+wQXmOdYNrMPPhltkrzRl2kSpo8bKRjWSLQefCle+33pgYsQljiAYriyBMpbEWpXZdRWAQ2sp8xYGDVLD4+M9KWduqSWgNIZoqu0c1S5f06Qe/+x7uXK/+Jt6irELDVSOcbP9GQrQcaiJ7r7LMmluJoy6Dc9oYBNFIIc2IV7/byfLeRy5UfvatryuXj2Vle/PtQMVdbBVVdTSefdRw4afq2gHVcDu5nnp8AJqys8pz1XXGnsQCPcNoRdBZGPGBgAEtYgQvrj3CjpRCXrp2PJdPjFCun9oK1TMENY5OdQl8fqPKhZ+6SJmYIxaoak1jONxjG1RGiZHR0tm4eqk0RMM9YRUjMIKP0ro4qKtgDD7n6t16Ty5okBpdc9cn6jojYk/tHCMXqnhAa3nzBlPuabIKS8xTVLaWeRIyTLm0pFRWaMKKplm1TgXDFVJwrG23kEG/McoPX1WsAv+tjvs0Tv2Nv/7MugPbg5OLcp215vo8GUPmqqSIS1/pMi4hg64lTW+iKFXlQPcbbWURFJbX8MnWVOaPDePiMWGqeGvrW8rdYow+GD5RuQqSNypFYmRkjLpMuS4O/6AKdb5dpHqpJ/N9nkl8+jb50S2rnS3TES0zhroKRvC63+jWX2LDaji6WrnSTrVhG3QuPJLS1PtujYAolem1433lSrIVKDYIGaHiUaXZKsOroV65EU8VS5naOp+BEE2WTRsZcQihBmk7/x+nLlNncMNXyqXaHsZdBw8faZ9F4WC0IugsilKVeeoXoWIE5mSp//12nIqaeu41JtHe9pZy70xbZL3/NHMR9tTfN/UGw2NVz23/12pIhpIMWPBGx+bfny7e/Wgcd92WReDkZjuPvbPxj1K9/LaCvo3uI3lqbiFL7JlUHtTvX12iXDXNq4otMayrH/6kLMQL/tG+++vqpVw8YJ8igCbLprWYSm+ki1kCBj1vYprugjFeiW+4cvtUFlJq8uH9TcmcP6IvQ/r6QO5h8zSIF7UMPA6ZC7euasoBB3Nu9gI1hrlsUMG/SBtDPXQmRsDY2UO5WRqXmy2CoCFdosCmBSYnuH1tyzF9LPEJVVZebbn1MM6OIHyimg86dVPbMhnxlkPLlXJqnht/KvhFKOvCHtcQqDkR7tygRpHVdGm6pnrqDRSnqerExnFcMvl4SyolVXUsOnuQKjxZdrcqzLr4hZY9RSGUr7d5D2PUZUoJhIxQef9dDSNo2Ly2wbOPsgbaciN0NiHD2s6yMkYq9Y2wDi47CsMqbKuH7hWsRrd081WBf3stDlsYRXT2WgSg0lI7Y9Y2zSmhLYLOoLpUZfz4928MwtUUpPHur87MHBzEmAh/+OU/qtLziveaetH2EDZB+YCHXdxySNyugKUisEQIVZ3cb9SZl6kjOfdJ5Yc/nQbXXobMhfP+3nbapRBw4f+prCJb+fangl87FIGmW6AVQWdgOcOR+eX6bedu8srG8Ps5g1Qu/YZn1As+6vJTO7YQTbMidUUMf7atPPRx155ZWRzBoFZqKByByQTT7z/5dqOv6JjzGRZBW64oTbdEu4Y6A6OGwD8KvPvSIJw5mHCIeWNCmTygj5qFC6lK3nsajRaBjdoGTddm0DkqzhDQBYP5mtNCWwSdgVFD4BfJwexy/Bv8Ge5Vyq1XjEUIocrQ+422v5CrO2EogrYqUzVdk7DxcOOyzpZC4wC0RdAZFKWAszs50pfbP9hOrimIGcFVeLg6qeGLM3e2PnBbd8c/UgUuu2L1sEbTS9EWwZnAmOXLSIssSqPBN4J7PtlFQUUNUYMH41porrY9sV+lk7a3erGr4+YDDx/uWrUNGk0vR1sEZ4KlN8EH85sm2ShKJbGmDztSCvnvlePw7xetBg+TsmlO1Z5qEYBKiT0TWTUajcYutCJwNCcOQsL3kPKrmnwDqMxLJr7Ii3tnxzBvTKhKIa2vVhOIpG1TxVWnm+qn0Wg0dqJdQ45m86vg4qly+je9zM7a/kyoKcA1KJqHzzfPqtRYVJauFEFPdQtpNJouiVYEjqQ0G/Z+roZ79lBFYj9kxzIBuGhGHE4ms3vEyM9Oj1dTIdqa+ESj0WgchHYNOZJtb6lp8abeC3F30mBy4ZrCtwDwDLEYythwAx38Vn325PiARqPpcmhF4ChqymH7uzD8YjV+vXcIu/tcQIzJPOern8XEGJ5BasKL5F/B2V3VEGg0Gs0ZQisCR7HrEzVS4zQ1BEBVbT1/zzcPTWxysR462GQyl+1LPUiXRqM542hF0NFIqaaVXPsERE5pDPyuOpDNrsq+5Eeca3uoZWMGKB0o1mg0ZxgdLO5Iqkvh+z/AvqUQPRMue7tx1efb04js40HADR9AfVXLfY3MIR0f0Gg0ZxitCDqSJdeoiUJmPwZn/bGx15+SX86mpHz+eP4QTO7egHfLfY2AcYS2CDQazZlFK4KOorJIFY2d9SeY/WerVZ9tT8Mk4MrYyFZ2BuLuVPEBPSqnRqM5w+gYQUeREa8+o6dbLc4vq+ajzSlcMLIffX3dW9/fpx8Mv8SBAmo0Go1ttCLoKNK2gTC1mNz85XWJVNTU8fD5QzpJMI1Go2kbrQg6irSt0HekGl3TTGp+BZ9sTeHqSZEMCvFpY2eNRqPpPLQi6Aga6iF9R4uMn2dXH8bJJHjwXG0NaDSarotWBB1BziGoKbXK+NmbXsR3ezK5fcbAtmMDGo1G08loRdARpG1VnxbFYC+uPUofL1fumjWwlZ00Go2ma6AVQUeQtk3NwRsQDUBtfQO/JeUxf2wYPu4unSubRqPRnAStCDqCdPMcAuZZtw5kllBV20DcgD6dLJhGo9GcHK0ITpeyXCg4ZuUWik8uACA2KqCzpNJoNBq70YrgdElvOcfw9uQCogI9CdFBYo1G0w1wqCIQQswVQhwWQiQKIR5pZZurhBAHhRAHhBBLHCmPQ0jbpoaVDh0HgJSS+ORCJkVrt5BGo+keOGysISGEE/AqcB6QDmwXQiyXUh602GYw8CgwXUpZKIQIcZQ8DiNtG4SNAxfV+z+WV05+eQ2TorVbSKPRdA8caRHEAYlSymNSyhrgM+DSZtvcAbwqpSwEkFLmOFCejqehAbJ2Q3hs46LG+IC2CDQaTTfBkYogHEiz+J5uXmbJEGCIEOI3IcQWIcRcWwcSQtwphIgXQsTn5uY6SNx2UJoJtRUQ3FQ5vD25kD5ergwM8upEwTQajcZ+OjtY7AwMBmYD1wJvCyH8m28kpXxLShkrpYwNDu5CwzTnHVWfgYMbF21PLiA2KgBhTiXVaDSaro4jFUEGYDkAf4R5mSXpwHIpZa2U8jhwBKUYugf5ieozcBAAOSVVpORX6ECxRqPpVjhSEWwHBgshBgghXIFrgOXNtvkGZQ0ghAhCuYqOOVCmjiU/EVy9Gyeij08pBCBWB4o1Gk03wmGKQEpZBywCVgGHgKVSygNCiKeFEPPNm60C8oUQB4H1wGIpZb6jZOpw8o5CYExjRfH25ALcXUyMCvfrZME0Go3Gfhw6VaWUciWwstmyv1n8L4E/mP+6H/mJEDGp8ev25ALGRwbg4tTZoReNRqOxH91itZfaKihKbYwPlFXXcTCzRLuFNBpNt0MrgvZSeByQEKRi27tTi2iQun5Ao9F0P7QiaC+NqaMxAMSnFCAEjO/fIvtVo9FoujRaEbSXZqmj8cmFDOvni6+ef0Cj0XQz7FIEQoivhRDzhBBacRjkJ4J3P3Dzoa6+gV2phXrYaY1G0y2xt2F/DbgOOCqEeEYIMdSBMnUP8hMb4wMJ2aWU19TrQLFGo+mW2KUIpJRrpZTXAxOAZGCtEGKTEOIWIUTv9IUYNQSotFHQgWKNRtM9sdvVI4QIBH4H3A7sAl5EKYY1DpGsK1NRAJUFjWMMxacUEubnTri/RycLptFoNKeOXQVlQohlwFDgI+ASKWWWedXnQoh4RwnXZbEIFKuJaAqIGxDYuTJpNBpNO7G3svglKeV6WyuklLG2lvdoDEUQNJj0wkpOlFTriWg0Gk23xV7X0AjL4aGFEAFCiHsdJFPXJ+8omJzBvz/xKSo+MFFnDGk0mm6KvYrgDillkfHFPKPYHY4RqRuQnwgBA8DJhfjkQrzdnBnWz7ezpdJoNJp2Ya8icBIWM62Y5yN2dYxI3YD8RKtCsvH9/XEy6YloNBpN98ReRfAjKjB8jhDiHOBT87LeR10N5CdBYAwlVbUcySnVbiGNRtOtsTdY/GfgLuAe8/c1wDsOkairc2wD1FdD9Az2phUjJUzorxWBRqPpvtilCKSUDcDr5r/ezYFl4OYHMWez6+dUAMZG6oHmNBpN98XeOoLBwP8DRgDuxnIp5UAHydU1qauGhBUw/GJwdmNXWhGDQrzx8+idxdUajaZnYG+M4H8oa6AOmAN8CHzsKKG6LIk/QXUxjLwMKSW7UguZoIed1mg03Rx7FYGHlPInQEgpU6SUTwLzHCdWF+XAMvAIgIGzSMmvoLCilvE6PqDRaLo59gaLq81DUB8VQiwCMgBvx4nVBamthMMrYdRl4OTCrrQTgJ6IRqPRdH/stQgeADyB+4GJwA3AzY4SqktydA3UlMHIywDYlVqEl6sTg0N8OlkwjUajOT1OahGYi8eullL+ESgDbnG4VF2RA8vAMwiiZwKwM7WQsZG6kEyj0XR/TmoRSCnrgRlnQJauS20VHPkRRswHJ2cqa+o5lFWq3UIajaZHYG+MYJcQYjnwBVBuLJRSfu0QqboaJRlQWwGRkwHYl1FMfYNkfKQOFGs0mu6PvYrAHcgHzrZYJoHeoQjKctSnVzAAu1ILARinLQKNRtMDsLeyuHfGBQzKVIYQ3n0BFSju38eTIG+3ThRKo9FoOgZ7K4v/h7IArJBS3trhEnVFynPVp3eIKiRLK2TKQD0jmUaj6RnY6xr63uJ/d2AhkNnx4nRRynJAmMAzkIwiNSPZeD2+kEaj6SHY6xr6yvK7EOJT4FeHSNQVKc8Bz0AwObEjRcUHYqP7dLJQGo1G0zHYW1DWnMFASEcK0qUpywUvdbnxyYV4ujoxrJ8uJNNoND0De2MEpVjHCLJRcxT0DspzwFtlDMWnqBnJnJ3aq0M1Go2ma2Gva6h3d3/LciByMqVVtRzOLmHR2YM7WyKNRqPpMOzq1gohFgoh/Cy++wshFjhOrC5GeS54h7ArtYgGCbF6akqNRtODsNe/8YSUstj4IqUsAp5wjEhdjOoyVVXsFUx8SiEmoUcc1Wg0PQt7FYGt7exNPe3eNBaThbAjpYCh/Xzxcdczkmk0mp6DvYogXgjxoSYwtwAAEB5JREFUnBAixvz3HLDjZDsJIeYKIQ4LIRKFEI+0sd3lQggphIi1V/AzhrmYrN4zmF2pRdotpNFoehz2KoL7gBrgc+AzoAr4fVs7mIevfhW4EDXX8bVCiBE2tvNBzXew1X6xzyDmcYaOV3lRUVNPbLRWBBqNpmdhb9ZQOdBqj74V4oBEKeUxACHEZ8ClwMFm2/0d+Dew+BSPf2YoV4pgZ75yB03UFoFGo+lh2Js1tEYI4W/xPUAIseoku4UDaRbf083LLI87AYiUUq44yfnvFELECyHic3Nz7RG54yjLBQS/ZUE/X3fC/T3O7Pk1Go3GwdjrGgoyZwoBIKUs5DQri81zID8HPHyybaWUb0kpY6WUscHBwadz2lOnPAc8+7A9pYSJ0QEIoWck02g0PQt7FUGDEKK/8UUIEY2N0UibkQFEWnyPMC8z8AFGARuEEMnAFGB5lwsYl+VQ5xFEZnEVE/prt5BGo+l52JsC+hfgVyHEz4AAZgJ3nmSf7cBgIcQAlAK4BrjOWGmuSwgyvgshNgB/lFLG2y39maA8l3IXNeR0TLBXJwuj0Wg0HY9dFoGU8kcgFjgMfIpy51SeZJ86YBGwCjgELJVSHhBCPC2EmH9aUp9JynIoNqnwSHSgVgQajabnYe+gc7ejUjwjgN0oN85mrKeubIGUciWwstmyv7Wy7Wx7ZDnjlOWQ6z4ZJ5MgPEAHijUaTc/D3hjBA8AkIEVKOQcYDxS1vUsPoKYcasvJqPUlzN8dFz3iqEaj6YHY27JVSSmrAIQQblLKBGCo48TqIhjFZNVeRPXRbiGNRtMzsTdYnG6uI/gGWCOEKARSHCdWF8E8vMSRMg/6D/TsZGE0Go3GMdhbWbzQ/O+TQoj1gB/wo8Ok6iqYLYLkKi8uDdSKQKPR9ExOeQRRKeXPjhCkS2IeXiJP+tFfu4Y0Gk0PRUc/26JMuYby8SVKWwQajaaHohVBW5TnUOXsRx3O9O+jFYFGo+mZaEXQFmU5FDv5E+zjhpdb75iHR6PR9D60ImiL8lzypB9R2hrQaDQ9GK0I2qLsBJl1PvTX8QGNRtOD0YqgDWRZDmk1PrqYTKPR9Gi0ImiN2kpETRl50o/oIG0RaDSanotWBK1hLibLxU9nDGk0mh6NVgStkaOmVk5p6EuUHn5ao9H0YLQiaI20rdTjxDHXoQR4unS2NBqNRuMwdHJ8a6RtI8V1EH29/fU8xRqNpkejLQJb1NdCxk52yiF6VjKNRtPj0RaBLbL3QV0lv9QN0DUEGo2mx6MtAlukbQNgW91gXVWs0Wh6PFoR2CJtK5Ue/cgmkBFhvp0tjUaj0TgUrQhskb6dJLcReLo6MSJUKwKNRtOz0YqgOcUZUJzGbzUxjIv0x1lPWK/RaHo4upVrTrqKD/xQ1J/Y6D6dLIxGo9E4Hq0ImpO2jXonNw40RBEbFdDZ0mg0Go3D0YqgOWnbyPIaQb1wZnx//86WRqPRaByOVgSW1FZC1h52ysEM6+eLj7seWkKj0fR8tCKwJGsvNNSyurg/k6K1W0ij0fQOtCKwJOcAALtrI5moA8UajaaXoBWBJTkJ1Dp5kiEDtUWg0Wh6DVoRWJJ7iAzn/oT5exHq59HZ0mg0Gs0ZQSsCC2TOIfbWhBGrrQGNRtOL0KOPGpTnI8pz2VMbqusHNBpNr0JbBAa5hwA4KiMYE6HrBzQaTe9BKwKDHKUIjjREMCjEu5OF0Wg0mjOHQxWBEGKuEOKwECJRCPGIjfV/EEIcFELsFUL8JISIcqQ8bZJziEqTFybfMLzctMdMo9H0HhymCIQQTsCrwIXACOBaIcSIZpvtAmKllGOAL4H/c5Q8JyU3geOm/sT09ek0ETQajaYzcKRFEAckSimPSSlrgM+ASy03kFKul1JWmL9uASIcKE/rSInMOcT+mjDtFtJoNL0ORyqCcCDN4nu6eVlr3Ab8YGuFEOJOIUS8ECI+Nze3A0U0U5aDqCzgYH24VgQajabX0SWCxUKIG4BY4Flb66WUb0kpY6WUscHBwR0vgDlj6IiMYFCwVgQajaZ34cioaAYQafE9wrzMCiHEucBfgFlSymoHytM6OQkAHGmI1BaBRqPpdTjSItgODBZCDBBCuALXAMstNxBCjAfeBOZLKXMcKEvb5Byk3MmPOo9A+ni5dpoYGo1G0xk4TBFIKeuARcAq4BCwVEp5QAjxtBBivnmzZwFv4AshxG4hxPJWDudYchNIMfVnUIgPQohOEUGj0Wg6C4cmzEspVwIrmy37m8X/5zry/HYhJeQksL9uinYLaTSaXomunCrJhOpi9tbq1FGNRtM76RJZQ52KMcZQQwQxWhFoNJpeiFYERsaQDNepoxrN/2/v7mPrruo4jr8/e4Q9SDcY3WwH29yYdIQnFwKChoDDgYbxB0QUkRgi/2AEY6IQn/mPxIgaCUIAHUiQMIcuhIy5QTD8wcOACWu7JyaMkbGWsRUGlD59/eN3iteulQ56e7ff+bySpvf3u7/ee07P7f30d37nnmNZchC0tbJ/3HQ6x0+joc6L0ZhZfhwE7a28OvY45s2YzJgxHjFkZvnJOwj6+qB9M809nlrCzPKV96ihjtegaz8vdM/09QEzy1beZwTt/VNLeDEaM8tX3kHQ9t/lKR0EZpar7IPg7fEz6Bw7lTnHTK51aczMaiLvIGhvZUcaMTR+bN6/CjPLV77vfn290L6Fl7obOMHLU5pZxvIdNbT3Feh5n+e7Z7JwpoPAzPKV7xlBGjG0ta/RZwRmlrV8g+DDEUMNLHQQmFnG8u0aamtl34SZ9PVOoXGa5xgys3zle0bQvolXxhzHgvopnmPIzLKWZxD09sCbW3ixa5avD5hZ9vIMgre2Q28XGzpn+fqAmWUvzyBoawFgSzRygoeOmlnm8gyCrWvoGjeVLTHbZwRmlr38Rg31fACtD7Nx6jlM7DmS+k9NrHWJzMxqKr8zgpcfhw86eJSzWFg/Fckjhswsb/kFQfNK4og6Vuyd7+sDZmbkFgTdnbDpETrnX8SeTnx9wMyM3IJg21roeodtxy4B8GcIzMzILQiaV8Kko1n7/kLGCE6c5SAwM8snCLreg82r4cSLWd2yh8VzplM3aUKtS2VmVnP5BMHWNdD9LrtmX8jm3e+wdNHMWpfIzOyQkE8Q9HZDw2JW7ZsLwAWL6mtcIDOzQ0M+QXDyZfCddaxuaeekhk/ROG1SrUtkZnZIyCcIgN1vd/LCjn18ucndQmZm/bIKgjXNbwCw9CQHgZlZv6yC4NHm3cw7ZjLzj51S66KYmR0ysgmCjve6eWr7Hi5YNNPzC5mZVahqEEhaKmmzpG2Sbhjk/omSHkj3Py1pTrXKsm7Tbnr6wt1CZmYDVC0IJI0FbgUuBJqAr0tqGnDY1cDeiJgP3ALcXK3yTD1iPEua6jm54ahqPYWZ2WGpmusRnAFsi4jtAJL+AiwDWiqOWQb8It1eAfxekiIiRrowS5rqWdLkzw6YmQ1Uza6hBuC1iu2dad+gx0RED9ABHD3wgSRdI2m9pPXt7e1VKq6ZWZ4Oi4vFEXFHRCyOiMUzZsyodXHMzEqlmkHwOjC7Yrsx7Rv0GEnjgKOAPVUsk5mZDVDNIHgWWCBprqQJwOXAqgHHrAKuSrcvBR6rxvUBMzMbWtUuFkdEj6TvAo8CY4G7I6JZ0k3A+ohYBdwF3CtpG/AWRViYmdkoquaoISLiEeCRAft+VnG7E7ismmUwM7P/77C4WGxmZtXjIDAzy5wOt2uzktqBVz/mjx8DvDmCxTlc5FjvHOsMedY7xzrDwdf7+IgYdPz9YRcEn4Sk9RGxuNblGG051jvHOkOe9c6xzjCy9XbXkJlZ5hwEZmaZyy0I7qh1AWokx3rnWGfIs9451hlGsN5ZXSMwM7MD5XZGYGZmAzgIzMwyl00QfNSymWUgabakxyW1SGqWdF3aP13SPyRtTd+n1bqsI03SWEkvSHo4bc9Ny59uS8uhTqh1GUeapDpJKyRtktQq6axM2vr76fW9UdL9ko4oW3tLultSm6SNFfsGbVsVfpfq/qKk0w/2+bIIgmEum1kGPcAPIqIJOBO4NtXzBmBdRCwA1qXtsrkOaK3Yvhm4JS2DupdiWdSy+S2wOiI+C5xCUf9St7WkBuB7wOKIOIliQsvLKV97/wlYOmDfUG17IbAgfV0D3HawT5ZFEFCxbGZEdAH9y2aWSkTsiojn0+13KN4YGijqujwdthy4pDYlrA5JjcBXgDvTtoDzKJY/hXLW+SjgixQz+BIRXRGxj5K3dTIOODKtYTIJ2EXJ2jsi/kkxI3Olodp2GXBPFJ4C6iTNOpjnyyUIhrNsZqlImgOcBjwN1EfErnTXG0DZFm/+DfBDoC9tHw3sS8ufQjnbey7QDvwxdYndKWkyJW/riHgd+BWwgyIAOoDnKH97w9Bt+4nf33IJgqxImgL8Fbg+It6uvC8t/FOaMcOSvgq0RcRztS7LKBsHnA7cFhGnAe8yoBuobG0NkPrFl1EE4aeByRzYhVJ6I922uQTBcJbNLAVJ4ylC4L6IWJl27+4/VUzf22pVvio4G7hY0isUXX7nUfSd16WuAyhne+8EdkbE02l7BUUwlLmtAb4E/Dsi2iOiG1hJ8Rooe3vD0G37id/fcgmC4SybedhLfeN3Aa0R8euKuyqXBL0K+Ptol61aIuLGiGiMiDkU7fpYRFwBPE6x/CmUrM4AEfEG8JqkhWnX+UALJW7rZAdwpqRJ6fXeX+9St3cyVNuuAr6VRg+dCXRUdCENT0Rk8QVcBGwBXgZ+XOvyVKmO51CcLr4IbEhfF1H0ma8DtgJrgem1LmuV6n8u8HC6PQ94BtgGPAhMrHX5qlDfU4H1qb3/BkzLoa2BXwKbgI3AvcDEsrU3cD/FNZBuirO/q4dqW0AUoyJfBl6iGFF1UM/nKSbMzDKXS9eQmZkNwUFgZpY5B4GZWeYcBGZmmXMQmJllzkFgNooknds/Q6rZocJBYGaWOQeB2SAkfVPSM5I2SLo9rXewX9ItaS78dZJmpGNPlfRUmgv+oYp54udLWivpX5Kel/SZ9PBTKtYRuC99QtasZhwEZgNIOhH4GnB2RJwK9AJXUExwtj4iFgFPAD9PP3IP8KOIOJnik539++8Dbo2IU4DPU3xSFIpZYa+nWBtjHsVcOWY1M+6jDzHLzvnA54Bn0z/rR1JM8NUHPJCO+TOwMq0LUBcRT6T9y4EHJU0FGiLiIYCI6ARIj/dMROxM2xuAOcCT1a+W2eAcBGYHErA8Im78n53STwcc93HnZ/mg4nYv/ju0GnPXkNmB1gGXSjoWPlwr9niKv5f+GS6/ATwZER3AXklfSPuvBJ6IYoW4nZIuSY8xUdKkUa2F2TD5PxGzASKiRdJPgDWSxlDMAHktxeIvZ6T72iiuI0AxJfAf0hv9duDbaf+VwO2SbkqPcdkoVsNs2Dz7qNkwSdofEVNqXQ6zkeauITOzzPmMwMwscz4jMDPLnIPAzCxzDgIzs8w5CMzMMucgMDPL3H8AS/s3gZRVz1IAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCXE1Nm45huL",
        "colab_type": "code",
        "outputId": "00c07f78-e5de-4fc2-8271-971a557f03c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Evaluate model\n",
        "train_score = model.evaluate(train_generator, verbose=1)\n",
        "print(\"Training loss: \", train_score[0])\n",
        "print(\"Training accuracy: \", train_score[1])\n",
        "\n",
        "test_score = model.evaluate(test_generator, verbose=1)\n",
        "print(\"Testing loss: \", test_score[0])\n",
        "print(\"Testing accuracy: \", test_score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 194s 966ms/step - loss: 0.3785 - accuracy: 0.9161\n",
            "Training loss:  0.3785020709037781\n",
            "Training accuracy:  0.9161078929901123\n",
            "844/844 [==============================] - 26s 31ms/step - loss: 0.6999 - accuracy: 0.8389\n",
            "Testing loss:  0.6998926401138306\n",
            "Testing accuracy:  0.8388625383377075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PJ6NpZE8AzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict test images\n",
        "predictions = []\n",
        "\n",
        "for filename in test_generator.filenames:\n",
        "    img = load_img(test_dir+filename, target_size=(image_width, image_height))\n",
        "    img = img_to_array(img)/255\n",
        "    img_expand = np.expand_dims(img, axis=0)\n",
        "    predictions.append(model.predict(img_expand)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHvvZGaHKQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get index of largest probability\n",
        "predicted_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get coin directory name from index \n",
        "directories = dict((v, k) for k, v in train_generator.class_indices.items())\n",
        "predicted_dir = [directories.get(k) for k in predicted_indices]\n",
        "\n",
        "# Get label name from coin directory name\n",
        "with open(data_dir + 'cat_to_name.json', 'r') as json_file:\n",
        "    labels = json.load(json_file)\n",
        "predicted_labels = [labels.get(str(k)) for k in predicted_dir]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4DLt4o3H78s",
        "colab_type": "code",
        "outputId": "f35ea157-5c2b-44ce-901a-64cfdad5f631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Save predicted labels as CSV file\n",
        "filenames = test_generator.filenames\n",
        "results = pd.DataFrame({\"Filename\": filenames, \"Predictions\": predicted_labels})\n",
        "results.to_csv(\"xception_results.csv\", index=False)\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/021__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/022__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/027__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/036__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/005__5 Centavos_brazil.jpg</td>\n",
              "      <td>5 Centavos,Brazilian Real,brazil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Filename                         Predictions\n",
              "0    1/021__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "1    1/022__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "2    1/027__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "3    1/036__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "4  10/005__5 Centavos_brazil.jpg    5 Centavos,Brazilian Real,brazil"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZlj71Kl_2WO",
        "colab_type": "text"
      },
      "source": [
        "# **Convert to TFLite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVWQuD08_1hc",
        "colab_type": "code",
        "outputId": "d56eaa5c-e155-454a-8c7d-2311800f0049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "open(\"xception_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87790352"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}