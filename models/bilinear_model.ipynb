{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bilinear_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vev5DAYLK5OZ",
        "colab_type": "text"
      },
      "source": [
        "# **Bangkit Final Project: World Coin Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aF2mIOQNC5o",
        "colab_type": "text"
      },
      "source": [
        "# **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu8dZbIdLrLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import zipfile\n",
        "\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential, Model, model_from_json, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, LeakyReLU\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Lambda, Reshape, concatenate\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.regularizers import Regularizer, l2\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0MskslwLsYt",
        "colab_type": "code",
        "outputId": "1eca41a9-917b-4d0c-802a-17c3ae28578e",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Upload the kaggle.json file from Kaggle account settings page\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c16fb1c8-c4aa-4c42-b10f-023309250a86\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c16fb1c8-c4aa-4c42-b10f-023309250a86\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-mZ_XHMezh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the Kaggle API client\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSwixL0MUxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo_rjYl-NU0_",
        "colab_type": "text"
      },
      "source": [
        "# **Importing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS2VsR0CNg0G",
        "colab_type": "code",
        "outputId": "44f44433-cfbd-4ef8-c077-2ec88db46a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d wanderdust/coin-images"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading coin-images.zip to /content\n",
            " 98% 449M/459M [00:09<00:00, 43.2MB/s]\n",
            "100% 459M/459M [00:09<00:00, 48.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN_bT4N7OopU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip the dataset into folder\n",
        "zip_ref = zipfile.ZipFile('/content/coin-images.zip', 'r')\n",
        "zip_ref.extractall('/content/')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2d5YlhRQpK",
        "colab_type": "text"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGaQrJvGRTG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define directories\n",
        "data_dir = \"/content/coins/data/\"\n",
        "\n",
        "train_dir = data_dir + \"train/\"\n",
        "validation_dir = data_dir + \"validation/\"\n",
        "test_dir = data_dir + \"test/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZoeHe2hJ-SF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=False,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      featurewise_std_normalization=False,\n",
        "      samplewise_std_normalization=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0BDiwC6MJcE",
        "colab_type": "code",
        "outputId": "8b11e061-40cb-4942-f7ba-c4b128d26286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Read images from generators\n",
        "batch_size = 32\n",
        "image_width = 299\n",
        "image_height = 299\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "      validation_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "      test_dir,\n",
        "      target_size=(image_width, image_height),\n",
        "      class_mode=\"categorical\",\n",
        "      batch_size=1\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 6413 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n",
            "Found 844 images belonging to 211 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9QD8ft22wk_",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX1T4tt0OrhX",
        "colab_type": "code",
        "outputId": "7feafedc-e40c-44cd-ded5-fb0af5b1141d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load base model\n",
        "base_model = Xception(\n",
        "    input_shape=(image_width, image_height, 3),\n",
        "    weights='imagenet',\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "    # Add regularizer\n",
        "    l2_layer = l2(0.01)\n",
        "    if hasattr(layer, 'kernel'):\n",
        "        base_model.add_loss(lambda layer=layer: l2_layer(layer.kernel))\n",
        "\n",
        "for layer in base_model.layers[:10]:\n",
        "\t\tlayer.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "# Reduce dimension\n",
        "x = Conv2D(128, (1,1), activation='relu')(x)\n",
        "\n",
        "# Bilinear pooling\n",
        "\n",
        "# Reshape to (minibatch_size, total_pixels, filter_size)\n",
        "x_detector, shape_detector = x, x.shape\n",
        "x_detector = Reshape([shape_detector[1] * shape_detector[2], shape_detector[-1]])(x_detector)\n",
        "x_extractor, shape_extractor = x, x.shape\n",
        "x_extractor = Reshape([shape_extractor[1] * shape_extractor[2], shape_extractor[-1]])(x_extractor)\n",
        "\n",
        "# Outer product\n",
        "def outer_product(x):\n",
        "    return backend.batch_dot(x[0], x[1], axes=[1,1]) / x[0].get_shape().as_list()[1]\n",
        "x = Lambda(outer_product)([x_detector, x_extractor])\n",
        "\n",
        "# Reshape to (minibatch_size, filter_size_detector*filter_size_extractor)\n",
        "x = Reshape([shape_detector[-1]*shape_extractor[-1]])(x)\n",
        "\n",
        "# Signed square root\n",
        "def signed_sqrt(x):\n",
        "    return backend.sign(x) * backend.sqrt(backend.abs(x)+1e-9)\n",
        "x = Lambda(signed_sqrt)(x)\n",
        "\n",
        "# L2 normalisation\n",
        "def L2_norm(x, axis=-1):\n",
        "    return backend.l2_normalize(x, axis=axis)\n",
        "x = Lambda(L2_norm)(x)\n",
        "\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(211, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.inputs, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 37, 37, 256)  32768       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 19, 19, 728)  186368      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 10, 10, 1024) 745472      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 10, 10, 128)  262272      block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100, 128)     0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 100, 128)     0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 128, 128)     0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 16384)        0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 16384)        0           reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 16384)        0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          8389120     lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 211)          108243      dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 29,621,115\n",
            "Trainable params: 29,538,075\n",
            "Non-trainable params: 83,040\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnhZ9bR-PiVb",
        "colab_type": "code",
        "outputId": "e107fd92-7e52-4f89-e49e-aebedaf1db2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Callback to reduce learning rate if no improvement in validation loss for certain number of epochs\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=1e-8, verbose=1)\n",
        "\n",
        "# Callback to stop training if no improvement in validation loss for certain number of epochs\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "# Callback to save best model weights per epoch\n",
        "weights_filepath = \"best_model_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=weights_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=50,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    validation_steps=3,\n",
        "    callbacks=[reduce_lr, checkpoint]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.3492 - accuracy: 0.0094\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.00000, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 101s 2s/step - loss: 5.3492 - accuracy: 0.0094 - val_loss: 5.3482 - val_accuracy: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.3268 - accuracy: 0.0209\n",
            "Epoch 00002: val_accuracy improved from 0.00000 to 0.01042, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 5.3268 - accuracy: 0.0209 - val_loss: 5.3362 - val_accuracy: 0.0104 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.2916 - accuracy: 0.0380\n",
            "Epoch 00003: val_accuracy improved from 0.01042 to 0.03125, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 98s 2s/step - loss: 5.2916 - accuracy: 0.0380 - val_loss: 5.3033 - val_accuracy: 0.0312 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.2271 - accuracy: 0.0631\n",
            "Epoch 00004: val_accuracy did not improve from 0.03125\n",
            "50/50 [==============================] - 98s 2s/step - loss: 5.2271 - accuracy: 0.0631 - val_loss: 5.2258 - val_accuracy: 0.0000e+00 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.1456 - accuracy: 0.0651\n",
            "Epoch 00005: val_accuracy improved from 0.03125 to 0.04167, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 5.1456 - accuracy: 0.0651 - val_loss: 5.0959 - val_accuracy: 0.0417 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 5.0112 - accuracy: 0.0800\n",
            "Epoch 00006: val_accuracy improved from 0.04167 to 0.08333, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 5.0112 - accuracy: 0.0800 - val_loss: 4.9966 - val_accuracy: 0.0833 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.8650 - accuracy: 0.0900\n",
            "Epoch 00007: val_accuracy did not improve from 0.08333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 4.8650 - accuracy: 0.0900 - val_loss: 4.9187 - val_accuracy: 0.0833 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.7018 - accuracy: 0.1246\n",
            "Epoch 00008: val_accuracy did not improve from 0.08333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 4.7018 - accuracy: 0.1246 - val_loss: 4.8289 - val_accuracy: 0.0625 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.5286 - accuracy: 0.1325\n",
            "Epoch 00009: val_accuracy did not improve from 0.08333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 4.5286 - accuracy: 0.1325 - val_loss: 4.5665 - val_accuracy: 0.0833 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.2881 - accuracy: 0.1831\n",
            "Epoch 00010: val_accuracy improved from 0.08333 to 0.14583, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 4.2881 - accuracy: 0.1831 - val_loss: 4.3459 - val_accuracy: 0.1458 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 4.0708 - accuracy: 0.2125\n",
            "Epoch 00011: val_accuracy did not improve from 0.14583\n",
            "50/50 [==============================] - 97s 2s/step - loss: 4.0708 - accuracy: 0.2125 - val_loss: 4.3251 - val_accuracy: 0.1354 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.9131 - accuracy: 0.2475\n",
            "Epoch 00012: val_accuracy improved from 0.14583 to 0.19792, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 3.9131 - accuracy: 0.2475 - val_loss: 3.9672 - val_accuracy: 0.1979 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.6524 - accuracy: 0.2456\n",
            "Epoch 00013: val_accuracy improved from 0.19792 to 0.20833, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 3.6524 - accuracy: 0.2456 - val_loss: 4.0194 - val_accuracy: 0.2083 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.4465 - accuracy: 0.3294\n",
            "Epoch 00014: val_accuracy improved from 0.20833 to 0.32292, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 3.4465 - accuracy: 0.3294 - val_loss: 3.5350 - val_accuracy: 0.3229 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.2604 - accuracy: 0.3593\n",
            "Epoch 00015: val_accuracy improved from 0.32292 to 0.34375, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 3.2604 - accuracy: 0.3593 - val_loss: 3.4415 - val_accuracy: 0.3438 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 3.0006 - accuracy: 0.3953\n",
            "Epoch 00016: val_accuracy improved from 0.34375 to 0.44792, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 3.0006 - accuracy: 0.3953 - val_loss: 3.1161 - val_accuracy: 0.4479 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.8461 - accuracy: 0.4371\n",
            "Epoch 00017: val_accuracy improved from 0.44792 to 0.52083, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 98s 2s/step - loss: 2.8461 - accuracy: 0.4371 - val_loss: 2.9568 - val_accuracy: 0.5208 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.6968 - accuracy: 0.4512\n",
            "Epoch 00018: val_accuracy improved from 0.52083 to 0.57292, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 2.6968 - accuracy: 0.4512 - val_loss: 2.7380 - val_accuracy: 0.5729 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.4805 - accuracy: 0.5016\n",
            "Epoch 00019: val_accuracy did not improve from 0.57292\n",
            "50/50 [==============================] - 97s 2s/step - loss: 2.4805 - accuracy: 0.5016 - val_loss: 2.6071 - val_accuracy: 0.5625 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.3323 - accuracy: 0.5250\n",
            "Epoch 00020: val_accuracy did not improve from 0.57292\n",
            "50/50 [==============================] - 97s 2s/step - loss: 2.3323 - accuracy: 0.5250 - val_loss: 2.4245 - val_accuracy: 0.5729 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.2143 - accuracy: 0.5475\n",
            "Epoch 00021: val_accuracy improved from 0.57292 to 0.60417, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 2.2143 - accuracy: 0.5475 - val_loss: 2.3762 - val_accuracy: 0.6042 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 2.0741 - accuracy: 0.5994\n",
            "Epoch 00022: val_accuracy improved from 0.60417 to 0.64583, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 2.0741 - accuracy: 0.5994 - val_loss: 2.1478 - val_accuracy: 0.6458 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.9031 - accuracy: 0.6180\n",
            "Epoch 00023: val_accuracy improved from 0.64583 to 0.65625, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.9031 - accuracy: 0.6180 - val_loss: 2.0685 - val_accuracy: 0.6562 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.8300 - accuracy: 0.6381\n",
            "Epoch 00024: val_accuracy improved from 0.65625 to 0.77083, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 100s 2s/step - loss: 1.8300 - accuracy: 0.6381 - val_loss: 1.8496 - val_accuracy: 0.7708 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.6879 - accuracy: 0.6731\n",
            "Epoch 00025: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.6879 - accuracy: 0.6731 - val_loss: 1.8840 - val_accuracy: 0.7083 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.5610 - accuracy: 0.6920\n",
            "Epoch 00026: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 97s 2s/step - loss: 1.5610 - accuracy: 0.6920 - val_loss: 2.1065 - val_accuracy: 0.6354 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.5042 - accuracy: 0.6988\n",
            "Epoch 00027: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.5042 - accuracy: 0.6988 - val_loss: 1.6600 - val_accuracy: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.4376 - accuracy: 0.7027\n",
            "Epoch 00028: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 97s 2s/step - loss: 1.4376 - accuracy: 0.7027 - val_loss: 1.4828 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2772 - accuracy: 0.7500\n",
            "Epoch 00029: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.2772 - accuracy: 0.7500 - val_loss: 1.5163 - val_accuracy: 0.6875 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.2279 - accuracy: 0.7531\n",
            "Epoch 00030: val_accuracy did not improve from 0.77083\n",
            "50/50 [==============================] - 98s 2s/step - loss: 1.2279 - accuracy: 0.7531 - val_loss: 1.3012 - val_accuracy: 0.7708 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.1549 - accuracy: 0.7653\n",
            "Epoch 00031: val_accuracy improved from 0.77083 to 0.78125, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 1.1549 - accuracy: 0.7653 - val_loss: 1.2095 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 1.0629 - accuracy: 0.7887\n",
            "Epoch 00032: val_accuracy improved from 0.78125 to 0.83333, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 1.0629 - accuracy: 0.7887 - val_loss: 1.2213 - val_accuracy: 0.8333 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9974 - accuracy: 0.8106\n",
            "Epoch 00033: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.9974 - accuracy: 0.8106 - val_loss: 1.4472 - val_accuracy: 0.6979 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.9311 - accuracy: 0.8210\n",
            "Epoch 00034: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.9311 - accuracy: 0.8210 - val_loss: 1.1256 - val_accuracy: 0.7604 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8974 - accuracy: 0.8150\n",
            "Epoch 00035: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.8974 - accuracy: 0.8150 - val_loss: 1.5607 - val_accuracy: 0.6979 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8871 - accuracy: 0.8181\n",
            "Epoch 00036: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.8871 - accuracy: 0.8181 - val_loss: 1.0904 - val_accuracy: 0.8333 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.8323 - accuracy: 0.8256\n",
            "Epoch 00037: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.8323 - accuracy: 0.8256 - val_loss: 0.8597 - val_accuracy: 0.8333 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.8419\n",
            "Epoch 00038: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.7877 - accuracy: 0.8419 - val_loss: 1.4736 - val_accuracy: 0.6771 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7452 - accuracy: 0.8552\n",
            "Epoch 00039: val_accuracy did not improve from 0.83333\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.7452 - accuracy: 0.8552 - val_loss: 0.9913 - val_accuracy: 0.8333 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.8612\n",
            "Epoch 00040: val_accuracy improved from 0.83333 to 0.86458, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.6866 - accuracy: 0.8612 - val_loss: 0.7772 - val_accuracy: 0.8646 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.7001 - accuracy: 0.8544\n",
            "Epoch 00041: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.7001 - accuracy: 0.8544 - val_loss: 0.8876 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5965 - accuracy: 0.8855\n",
            "Epoch 00042: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.5965 - accuracy: 0.8855 - val_loss: 0.7036 - val_accuracy: 0.8646 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.6201 - accuracy: 0.8731\n",
            "Epoch 00043: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.6201 - accuracy: 0.8731 - val_loss: 0.9092 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5740 - accuracy: 0.8863\n",
            "Epoch 00044: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.5740 - accuracy: 0.8863 - val_loss: 0.6918 - val_accuracy: 0.8333 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5430 - accuracy: 0.8963\n",
            "Epoch 00045: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.5430 - accuracy: 0.8963 - val_loss: 0.6516 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.8919\n",
            "Epoch 00046: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.5350 - accuracy: 0.8919 - val_loss: 1.0551 - val_accuracy: 0.7500 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.9069\n",
            "Epoch 00047: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.4756 - accuracy: 0.9069 - val_loss: 0.7172 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4726 - accuracy: 0.9087\n",
            "Epoch 00048: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.4726 - accuracy: 0.9087 - val_loss: 0.5292 - val_accuracy: 0.8438 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.9237\n",
            "Epoch 00049: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.4256 - accuracy: 0.9237 - val_loss: 1.0911 - val_accuracy: 0.7708 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3935 - accuracy: 0.9275\n",
            "Epoch 00050: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3935 - accuracy: 0.9275 - val_loss: 0.8582 - val_accuracy: 0.8021 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3946 - accuracy: 0.9244\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3946 - accuracy: 0.9244 - val_loss: 0.6361 - val_accuracy: 0.8542 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3915 - accuracy: 0.9244\n",
            "Epoch 00052: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3915 - accuracy: 0.9244 - val_loss: 0.6550 - val_accuracy: 0.8229 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3441 - accuracy: 0.9400\n",
            "Epoch 00053: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3441 - accuracy: 0.9400 - val_loss: 0.7098 - val_accuracy: 0.8646 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.9331\n",
            "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3598 - accuracy: 0.9331 - val_loss: 0.8039 - val_accuracy: 0.8333 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.9380\n",
            "Epoch 00055: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3433 - accuracy: 0.9380 - val_loss: 0.8956 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.9350\n",
            "Epoch 00056: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3400 - accuracy: 0.9350 - val_loss: 0.7502 - val_accuracy: 0.8125 - lr: 1.0000e-06\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.9481\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3340 - accuracy: 0.9481 - val_loss: 0.6364 - val_accuracy: 0.8333 - lr: 1.0000e-06\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.9406\n",
            "Epoch 00058: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3437 - accuracy: 0.9406 - val_loss: 0.6279 - val_accuracy: 0.8438 - lr: 1.0000e-07\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3428 - accuracy: 0.9419\n",
            "Epoch 00059: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3428 - accuracy: 0.9419 - val_loss: 0.8313 - val_accuracy: 0.8333 - lr: 1.0000e-07\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.9342\n",
            "Epoch 00060: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 96s 2s/step - loss: 0.3481 - accuracy: 0.9342 - val_loss: 0.6985 - val_accuracy: 0.8021 - lr: 1.0000e-07\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.9394\n",
            "Epoch 00061: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3451 - accuracy: 0.9394 - val_loss: 1.0967 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.9425\n",
            "Epoch 00062: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3343 - accuracy: 0.9425 - val_loss: 1.1571 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.9443\n",
            "Epoch 00063: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3335 - accuracy: 0.9443 - val_loss: 0.6758 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.9336\n",
            "Epoch 00064: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3455 - accuracy: 0.9336 - val_loss: 0.9362 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3491 - accuracy: 0.9388\n",
            "Epoch 00065: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3491 - accuracy: 0.9388 - val_loss: 0.7583 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.9438\n",
            "Epoch 00066: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3275 - accuracy: 0.9438 - val_loss: 0.8510 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3269 - accuracy: 0.9494\n",
            "Epoch 00067: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3269 - accuracy: 0.9494 - val_loss: 0.7520 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.9456\n",
            "Epoch 00068: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3355 - accuracy: 0.9456 - val_loss: 1.1532 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3383 - accuracy: 0.9400\n",
            "Epoch 00069: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3383 - accuracy: 0.9400 - val_loss: 0.6229 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9450\n",
            "Epoch 00070: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3266 - accuracy: 0.9450 - val_loss: 0.9413 - val_accuracy: 0.7708 - lr: 1.0000e-08\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.9304\n",
            "Epoch 00071: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3605 - accuracy: 0.9304 - val_loss: 1.0600 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3366 - accuracy: 0.9425\n",
            "Epoch 00072: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3366 - accuracy: 0.9425 - val_loss: 0.7667 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.9393\n",
            "Epoch 00073: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3410 - accuracy: 0.9393 - val_loss: 0.9273 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.9450\n",
            "Epoch 00074: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3455 - accuracy: 0.9450 - val_loss: 0.7632 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3411 - accuracy: 0.9400\n",
            "Epoch 00075: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3411 - accuracy: 0.9400 - val_loss: 0.9190 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3208 - accuracy: 0.9488\n",
            "Epoch 00076: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3208 - accuracy: 0.9488 - val_loss: 0.8259 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3311 - accuracy: 0.9444\n",
            "Epoch 00077: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3311 - accuracy: 0.9444 - val_loss: 0.7851 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.9475\n",
            "Epoch 00078: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3283 - accuracy: 0.9475 - val_loss: 0.5604 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3335 - accuracy: 0.9456\n",
            "Epoch 00079: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3335 - accuracy: 0.9456 - val_loss: 0.8104 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3372 - accuracy: 0.9413\n",
            "Epoch 00080: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3372 - accuracy: 0.9413 - val_loss: 0.6760 - val_accuracy: 0.8438 - lr: 1.0000e-08\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.9369\n",
            "Epoch 00081: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3512 - accuracy: 0.9369 - val_loss: 0.7228 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.9380\n",
            "Epoch 00082: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3327 - accuracy: 0.9380 - val_loss: 0.8191 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3493 - accuracy: 0.9406\n",
            "Epoch 00083: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3493 - accuracy: 0.9406 - val_loss: 0.8293 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.9431\n",
            "Epoch 00084: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3370 - accuracy: 0.9431 - val_loss: 0.7265 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.9362\n",
            "Epoch 00085: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3406 - accuracy: 0.9362 - val_loss: 1.0551 - val_accuracy: 0.7604 - lr: 1.0000e-08\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3341 - accuracy: 0.9450\n",
            "Epoch 00086: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3341 - accuracy: 0.9450 - val_loss: 0.8584 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3284 - accuracy: 0.9450\n",
            "Epoch 00087: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3284 - accuracy: 0.9450 - val_loss: 0.9890 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.9481\n",
            "Epoch 00088: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3299 - accuracy: 0.9481 - val_loss: 0.7151 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.9419\n",
            "Epoch 00089: val_accuracy did not improve from 0.86458\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3354 - accuracy: 0.9419 - val_loss: 0.9246 - val_accuracy: 0.7812 - lr: 1.0000e-08\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9450\n",
            "Epoch 00090: val_accuracy improved from 0.86458 to 0.87500, saving model to best_model_weights.hdf5\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3294 - accuracy: 0.9450 - val_loss: 0.5078 - val_accuracy: 0.8750 - lr: 1.0000e-08\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3447 - accuracy: 0.9362\n",
            "Epoch 00091: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3447 - accuracy: 0.9362 - val_loss: 0.7459 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.9356\n",
            "Epoch 00092: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3495 - accuracy: 0.9356 - val_loss: 0.8855 - val_accuracy: 0.7500 - lr: 1.0000e-08\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3281 - accuracy: 0.9494\n",
            "Epoch 00093: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3281 - accuracy: 0.9494 - val_loss: 0.8848 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9388\n",
            "Epoch 00094: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3282 - accuracy: 0.9388 - val_loss: 0.7738 - val_accuracy: 0.8333 - lr: 1.0000e-08\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.9525\n",
            "Epoch 00095: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 97s 2s/step - loss: 0.3242 - accuracy: 0.9525 - val_loss: 0.8365 - val_accuracy: 0.8229 - lr: 1.0000e-08\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3471 - accuracy: 0.9394\n",
            "Epoch 00096: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3471 - accuracy: 0.9394 - val_loss: 0.8383 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3619 - accuracy: 0.9319\n",
            "Epoch 00097: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 98s 2s/step - loss: 0.3619 - accuracy: 0.9319 - val_loss: 1.0964 - val_accuracy: 0.7917 - lr: 1.0000e-08\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.9463\n",
            "Epoch 00098: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 101s 2s/step - loss: 0.3332 - accuracy: 0.9463 - val_loss: 1.0163 - val_accuracy: 0.8125 - lr: 1.0000e-08\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.9431\n",
            "Epoch 00099: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 100s 2s/step - loss: 0.3426 - accuracy: 0.9431 - val_loss: 0.8714 - val_accuracy: 0.8021 - lr: 1.0000e-08\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.9469\n",
            "Epoch 00100: val_accuracy did not improve from 0.87500\n",
            "50/50 [==============================] - 99s 2s/step - loss: 0.3444 - accuracy: 0.9469 - val_loss: 0.9818 - val_accuracy: 0.7917 - lr: 1.0000e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIvbXL32hN48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load best model weights\n",
        "model.load_weights(weights_filepath)\n",
        "\n",
        "# Save model\n",
        "model.save(\"bilinear_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZTPuGY8rWm",
        "colab_type": "code",
        "outputId": "0f978131-d4c4-450c-b19f-0b93b4fd0a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "# Visualise accuracy history\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Visualise loss history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCXE1Nm45huL",
        "colab_type": "code",
        "outputId": "331f81b4-5b64-4fe7-cfb6-e73f1024d511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Evaluate model\n",
        "train_score = model.evaluate(train_generator, verbose=1)\n",
        "print(\"Training loss: \", train_score[0])\n",
        "print(\"Training accuracy: \", train_score[1])\n",
        "\n",
        "test_score = model.evaluate(test_generator, verbose=1)\n",
        "print(\"Testing loss: \", test_score[0])\n",
        "print(\"Testing accuracy: \", test_score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201/201 [==============================] - 193s 961ms/step - loss: 0.2090 - accuracy: 0.9710\n",
            "Training loss:  0.209010511636734\n",
            "Training accuracy:  0.9709964394569397\n",
            "844/844 [==============================] - 27s 32ms/step - loss: 0.6681 - accuracy: 0.8590\n",
            "Testing loss:  0.6681042909622192\n",
            "Testing accuracy:  0.8590047359466553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PJ6NpZE8AzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict test images\n",
        "predictions = []\n",
        "\n",
        "for filename in test_generator.filenames:\n",
        "    img = load_img(test_dir+filename, target_size=(image_width, image_height))\n",
        "    img = img_to_array(img)/255\n",
        "    img_expand = np.expand_dims(img, axis=0)\n",
        "    predictions.append(model.predict(img_expand)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhHvvZGaHKQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get index of largest probability\n",
        "predicted_indices = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get coin directory name from index \n",
        "directories = dict((v, k) for k, v in train_generator.class_indices.items())\n",
        "predicted_dir = [directories.get(k) for k in predicted_indices]\n",
        "\n",
        "# Get label name from coin directory name\n",
        "with open(data_dir + 'cat_to_name.json', 'r') as json_file:\n",
        "    labels = json.load(json_file)\n",
        "predicted_labels = [labels.get(str(k)) for k in predicted_dir]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4DLt4o3H78s",
        "colab_type": "code",
        "outputId": "aefd5473-22ee-4c42-9d88-a22f7a395e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Save predicted labels as CSV file\n",
        "filenames = test_generator.filenames\n",
        "results = pd.DataFrame({\"Filename\": filenames, \"Predictions\": predicted_labels})\n",
        "results.to_csv(\"bilinear_results.csv\", index=False)\n",
        "results.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Predictions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1/021__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1/022__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1/027__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1/036__1 Cent_australia.jpg</td>\n",
              "      <td>1 Cent,Australian dollar,australia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10/005__5 Centavos_brazil.jpg</td>\n",
              "      <td>5 Centavos,Brazilian Real,brazil</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Filename                         Predictions\n",
              "0    1/021__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "1    1/022__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "2    1/027__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "3    1/036__1 Cent_australia.jpg  1 Cent,Australian dollar,australia\n",
              "4  10/005__5 Centavos_brazil.jpg    5 Centavos,Brazilian Real,brazil"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF69Ut9iKmJW",
        "colab_type": "text"
      },
      "source": [
        "# **Convert to TFLite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEOxwCVQKlpS",
        "colab_type": "code",
        "outputId": "414a933e-7fed-413b-cab2-abf16ea7c4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "open(\"bilinear_model.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118203312"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}